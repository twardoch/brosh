This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: varia, .specstory, AGENT.md, CLAUDE.md, PLAN.md, SPEC.md, llms.txt, .cursorrules
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    browser-management.mdc
    component-flow.mdc
    data-models.mdc
    screenshot-algorithms.mdc
.giga/
  specifications.json
.github/
  workflows/
    push.yml
    release.yml
src/
  brosh/
    __init__.py
    __main__.py
    brosh.py
    browser.py
    capture.py
    cli.py
    image.py
    mcp.py
    models.py
    optimize.py
    tool.py
tests/
  test_package.py
.cursorindexingignore
.gitignore
.pre-commit-config.yaml
CHANGELOG.md
cleanup.sh
LICENSE
package.toml
pyproject.toml
pyrightconfig.json
README.md
TODO.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/brosh/optimize.py">
#!/usr/bin/env python3
# this_file: src/brosh/optimize.py

"""Image optimization utilities for brosh."""

import os
from pathlib import Path
from typing import Union

try:
    import oxipng
except ImportError:
    oxipng = None

from loguru import logger
from PIL import Image


def optimize_png_file(
    file_path: str | Path,
    level: int = 6,
    preserve_file: bool = True,
) -> bytes:
    """Optimize a PNG file using pyoxipng.

    Args:
        file_path: Path to the PNG file to optimize
        level: Optimization level (0-6, default 6)
        preserve_file: If True, keeps the original file; if False, overwrites it

    Returns:
        Optimized PNG data as bytes
    """
    if oxipng is None:
        logger.warning("pyoxipng not available, skipping PNG optimization")
        with open(file_path, "rb") as f:
            return f.read()

    try:
        # Read the original file
        with open(file_path, "rb") as f:
            original_data = f.read()

        # Optimize the PNG data
        optimized_data = oxipng.optimize_from_memory(
            original_data,
            level=level,
            strip=oxipng.StripChunks.safe(),
            interlace=None,  # Keep existing interlacing
            optimize_alpha=True,
            fast_evaluation=False,
        )

        original_size = len(original_data)
        optimized_size = len(optimized_data)
        reduction_pct = 100 * (1 - optimized_size / original_size)

        logger.debug(f"PNG optimization: {original_size} -> {optimized_size} bytes ({reduction_pct:.1f}% reduction)")

        # Write back if not preserving original
        if not preserve_file:
            with open(file_path, "wb") as f:
                f.write(optimized_data)

        return optimized_data

    except Exception as e:
        logger.error(f"Failed to optimize PNG: {e}")
        with open(file_path, "rb") as f:
            return f.read()


def downsample_png_data(
    png_data: bytes,
    scale_factor: float = 0.5,
) -> bytes:
    """Downsample PNG data by a given scale factor.

    Args:
        png_data: Raw PNG data
        scale_factor: Scale factor (0.5 = 50% size)

    Returns:
        Downsampled PNG data as bytes
    """
    import io

    try:
        # Load image from bytes
        img = Image.open(io.BytesIO(png_data))

        # Calculate new dimensions
        new_width = int(img.width * scale_factor)
        new_height = int(img.height * scale_factor)

        # Resize with high-quality resampling
        resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

        # Save to bytes
        output = io.BytesIO()
        resized.save(output, format="PNG", optimize=True)
        output.seek(0)

        return output.read()

    except Exception as e:
        logger.error(f"Failed to downsample PNG: {e}")
        return png_data


def compress_html_content(html: str) -> str:
    """Compress HTML content by removing SVG elements and other optimizations.

    Args:
        html: Raw HTML content

    Returns:
        Compressed HTML content
    """
    import re

    # Remove SVG content but keep placeholder
    def replace_svg(match):
        svg_tag = match.group(0)
        # Extract width and height if present
        width_match = re.search(r'width=["\']([\d.]+)["\']', svg_tag)
        height_match = re.search(r'height=["\']([\d.]+)["\']', svg_tag)

        attrs = []
        if width_match:
            attrs.append(f'width="{width_match.group(1)}"')
        if height_match:
            attrs.append(f'height="{height_match.group(1)}"')

        attr_str = " " + " ".join(attrs) if attrs else ""
        return f"<svg{attr_str}></svg>"

    # Replace SVG elements
    html = re.sub(r"<svg[^>]*>.*?</svg>", replace_svg, html, flags=re.DOTALL | re.IGNORECASE)

    # Remove comments
    html = re.sub(r"<!--.*?-->", "", html, flags=re.DOTALL)

    # Remove excessive whitespace
    html = re.sub(r"\s+", " ", html)
    html = re.sub(r">\s+<", "><", html)

    # Remove inline styles if they're too long
    html = re.sub(r'style="[^"]{500,}"', 'style=""', html)

    # Remove data URIs
    html = re.sub(r'src="data:[^"]{100,}"', 'src=""', html)
    html = re.sub(r'href="data:[^"]{100,}"', 'href=""', html)

    return html.strip()


def calculate_result_size(result: dict) -> int:
    """Calculate the serialized size of a result dictionary.

    Args:
        result: Dictionary to calculate size for

    Returns:
        Size in bytes
    """
    import json

    try:
        # Serialize to JSON to get actual size
        json_str = json.dumps(result, ensure_ascii=False)
        return len(json_str.encode("utf-8"))
    except Exception as e:
        logger.error(f"Failed to calculate result size: {e}")
        return 0
</file>

<file path=".cursor/rules/browser-management.mdc">
---
description: Browser detection, connection management, and platform-specific handling in a screenshot automation tool
globs: src/brosh/browser.py,src/brosh/tool.py
alwaysApply: false
---


# browser-management

## Core Components (Importance: 85)

1. Platform-Specific Browser Detection
- Chrome/Edge/Safari priority system with OS-dependent availability
- Safari restricted to macOS environments
- Firefox explicitly excluded per business requirements
- Custom detection logic for remote debugging endpoints

2. Debug Mode Connection Management
- Dedicated debug ports per browser:
  - Chromium: 9222
  - Edge: 9223
  - WebKit: 9225
- Progressive connection strategy with fallbacks:
  - Attempt existing browser connection
  - Launch new debug instance if needed
  - Platform-specific cleanup procedures

## Display Management (Importance: 75)

1. Resolution Handling
- OS-specific screen dimension detection
- Special handling for Retina displays (physical resolution ÷ 2)
- Fallback resolution: 1440x900
- Full-page capture support (height=-1)

2. Zoom Control
- CSS-based zoom implementation vs device scaling
- Custom DOM event listeners for zoom application
- Validation constraints: 10-500% range

## Browser Session Management (Importance: 80)

1. Lifecycle Control
- Aggressive process cleanup before debug launches
- Platform-specific termination procedures
- Session persistence for authenticated captures
- Automatic recovery from disconnections

2. Viewport Management 
- Dynamic viewport resizing based on capture requirements
- Content-aware dimension calculations
- Platform-specific DPI handling

File Paths:
```
src/brosh/browser.py  # Primary browser management implementation
src/brosh/tool.py     # Integration with screenshot workflow
```

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga browser-management".
</file>

<file path=".cursor/rules/component-flow.mdc">
---
description: Analyzes data and control flow between core browser screenshot components, focusing on async operations and error handling
globs: src/brosh/browser.py,src/brosh/capture.py,src/brosh/image.py,src/brosh/tool.py
alwaysApply: false
---


# component-flow

Core component interaction flow for browser screenshot operations:

## Primary Component Flow (Importance: 95)

1. BrowserManager → CaptureManager
- Provides browser context and viewport configuration
- Handles browser lifecycle events and connection status
- Passes viewport dimensions and scroll capabilities

2. CaptureManager → ImageProcessor
- Sends sequential viewport captures during scrolling
- Provides scroll position metadata for frame alignment
- Transmits visible HTML content for section detection

3. ImageProcessor → BrowserManager
- Signals frame capture completion for scroll coordination
- Provides merged image dimensions for viewport updates
- Reports animation frame timing for scroll synchronization

## Error Flow Paths (Importance: 85)

1. Browser Connection Failures
- BrowserManager retries connection with exponential backoff
- Falls back to new browser instance launch
- Notifies CaptureManager of viewport reconfiguration

2. Capture Synchronization
- CaptureManager validates scroll state before captures
- Implements retry logic for failed frame captures
- Maintains capture session state across retries

3. Image Processing Failures 
- ImageProcessor maintains partial results for recovery
- Supports resuming from last successful frame
- Preserves capture metadata during retries

## Async Operation Flow (Importance: 80)

1. Progressive Capture Pipeline
- BrowserManager initiates async browser operation
- CaptureManager orchestrates parallel scroll and capture
- ImageProcessor handles concurrent frame processing

2. State Management
- Components maintain isolated state machines
- Async event coordination via message passing
- Viewport synchronization through state broadcasts

File paths:
- `src/brosh/browser.py`
- `src/brosh/capture.py`
- `src/brosh/image.py`
- `src/brosh/tool.py`

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga component-flow".
</file>

<file path=".cursor/rules/data-models.mdc">
---
description: Specification for data models and enums used in browser-based screenshot capture and browser management
globs: **/models.py,**/browser.py,**/capture.py
alwaysApply: false
---


# data-models

## Core Data Models
Importance Score: 95

### ImageFormat Enum
File: src/brosh/models.py
- Defines supported screenshot output formats:
  - PNG: Default lossless format
  - JPG: Compressed format
  - APNG: Animated format for scroll sequences
- Used for format validation and conversion logic

### Capture Configuration Models 
File: src/brosh/capture.py
- Defines viewport capture parameters:
  - scroll_step: Controls content overlap percentage (10-200)
  - zoom_factor: Browser zoom level (10-500)
  - scale_factor: Output image scaling (10-200)
  - section_identifiers: Maps DOM sections to semantic names

### Browser Management Structures
File: src/brosh/browser.py
- Browser debug connection configurations:
  - Debug port mappings per browser type
  - Screen resolution detection rules
  - Retina display handling (macOS)
- Browser preference hierarchy:
  Chrome > Edge > Safari (macOS only)
- Platform-specific browser constraints:
  - Safari restricted to macOS
  - Firefox explicitly unsupported

The data models implement specialized screenshot capture workflow configuration and browser management rules, focusing on viewport control, section identification, and cross-platform browser compatibility.

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga data-models".
</file>

<file path=".cursor/rules/screenshot-algorithms.mdc">
---
description: Specifies core screenshot capture algorithms including viewport scrolling, section detection and image merging.
globs: src/brosh/capture.py,src/brosh/tool.py,src/brosh/image.py
alwaysApply: false
---


# screenshot-algorithms

## Viewport Scrolling Algorithm (90/100)
Core screenshot capture logic implemented in `src/brosh/capture.py`:

1. Progressive Viewport Scrolling
- Calculates scroll positions based on viewport height and configurable overlap
- Handles dynamic content loading between scrolls
- Maintains scroll state consistency across captures
- Supports starting capture from specific DOM selectors

2. Section Detection (85/100)
- Analyzes visible headers and ID elements to identify meaningful page sections
- Generates semantic identifiers from visible content
- Implements viewport intersection detection for section boundaries
- Prioritizes selectors using predefined content patterns:
```python
['main', 'article', '[role="main"]', '.content', '#content']
```

3. Content Visibility Analysis (80/100)
- Custom algorithm for detecting fully visible elements in viewport
- Hierarchy-aware filtering to avoid duplicate content capture
- Excludes non-visible structural elements
- Handles partially visible elements at viewport boundaries

## Image Processing (75/100)
Screenshot merging and processing in `src/brosh/image.py`:

1. Frame Assembly
- Merges sequential viewport captures with overlap removal
- Maintains visual consistency across scroll transitions
- Custom APNG creation for scrolling animation
- Handles alpha channel preservation across formats

2. Business Rules
- Scroll step: 10-200% of viewport height
- Zoom range: 10-500%
- Required overlap between consecutive captures
- Section-based frame segmentation

## Key Differentiators
- Semantic section identification for contextual organization
- Intelligent viewport intersection detection
- Dynamic content-aware scrolling algorithm
- Automated section boundary detection

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga screenshot-algorithms".
</file>

<file path=".giga/specifications.json">
[
  {
    "fileName": "main-overview.mdc",
    "description": "Complete system overview detailing the architecture, component interactions, and core functionality of the browser screenshot tool"
  },
  {
    "fileName": "screenshot-algorithms.mdc",
    "description": "Detailed documentation of the core screenshot algorithms including viewport scrolling logic, section detection, and image merging processes"
  },
  {
    "fileName": "browser-management.mdc",
    "description": "Documentation of browser detection, connection management, and platform-specific handling including debug mode operations and cleanup procedures"
  },
  {
    "fileName": "data-models.mdc",
    "description": "Specification of data models including ImageFormat enum, capture configurations, and browser management structures"
  },
  {
    "fileName": "component-flow.mdc",
    "description": "Documentation of data and control flow between core components (BrowserManager, CaptureManager, ImageProcessor) including async operations and error handling"
  }
]
</file>

<file path=".github/workflows/push.yml">
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/brosh --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/brosh
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path="src/brosh/__init__.py">
#!/usr/bin/env python3
# this_file: src/brosh/__init__.py

"""Browser screenshot tool using Playwright async API."""

from .cli import BrowserScreenshotCLI
from .models import ImageFormat
from .tool import BrowserScreenshotTool

__version__ = "0.1.0"
__all__ = ["BrowserScreenshotCLI", "BrowserScreenshotTool", "ImageFormat"]
</file>

<file path="src/brosh/__main__.py">
#!/usr/bin/env python3
# this_file: src/brosh/__main__.py

"""CLI entry point for brosh."""

import fire

from .cli import BrowserScreenshotCLI


def main():
    """Main entry point for the brosh CLI."""
    fire.Fire(BrowserScreenshotCLI)


if __name__ == "__main__":
    main()
</file>

<file path="src/brosh/image.py">
#!/usr/bin/env python3
# this_file: src/brosh/image.py

"""Image processing utilities for brosh."""

from pathlib import Path

from loguru import logger
from PIL import Image

from .optimize import optimize_png_file


class ImageProcessor:
    """Handles image processing operations."""

    @staticmethod
    def scale_image(filepath: Path, scale: int) -> None:
        """Scale the image by the given percentage.

        Args:
            filepath: Path to the image file
            scale: Scale percentage (100 = no scaling)

        """
        try:
            img = Image.open(filepath)
            new_width = int(img.width * scale / 100)
            new_height = int(img.height * scale / 100)
            resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
            resized.save(filepath)
        except Exception as e:
            logger.error(f"Failed to scale image {filepath}: {e}")

    @staticmethod
    def convert_to_jpg(png_path: Path) -> Path:
        """Convert PNG to JPG format.

        Args:
            png_path: Path to PNG file

        Returns:
            Path to JPG file

        """
        try:
            jpg_path = png_path.with_suffix(".jpg")
            img = Image.open(png_path)

            # Convert RGBA to RGB for JPG
            if img.mode in ("RGBA", "LA", "P"):
                background = Image.new("RGB", img.size, (255, 255, 255))
                if img.mode == "P":
                    img = img.convert("RGBA")
                background.paste(img, mask=img.split()[-1] if img.mode == "RGBA" else None)
                img = background

            img.save(jpg_path, "JPEG", quality=90)
            png_path.unlink()  # Remove original PNG
            return jpg_path
        except Exception as e:
            logger.error(f"Failed to convert {png_path} to JPG: {e}")
            return png_path  # Return original if conversion fails

    @staticmethod
    def optimize_png(png_path: Path) -> None:
        """Optimize a PNG file in-place.

        Args:
            png_path: Path to PNG file to optimize
        """
        try:
            # Optimize the PNG file, overwriting the original
            optimize_png_file(png_path, level=6, preserve_file=False)
            logger.debug(f"Optimized PNG: {png_path}")
        except Exception as e:
            logger.error(f"Failed to optimize PNG {png_path}: {e}")

    @staticmethod
    def create_apng(
        png_paths: list[Path],
        domain: str,
        output_path: Path,
        anim_spf: float,
    ) -> Path:
        """Create an animated PNG from a list of PNG files.

        Args:
            png_paths: List of PNG file paths to combine
            domain: Domain name for output filename
            output_path: Output directory
            anim_spf: Seconds per frame

        Returns:
            Path to created APNG file

        """
        apng_path = output_path / f"{domain}-animated.png"

        try:
            # Load all images
            images = []
            for png_path in png_paths:
                img = Image.open(png_path)
                images.append(img)

            # Convert seconds per frame to milliseconds
            duration_ms = int(anim_spf * 1000)

            # Save as animated PNG
            if images:
                images[0].save(
                    apng_path,
                    format="PNG",
                    save_all=True,
                    append_images=images[1:],
                    duration=duration_ms,
                    loop=0,  # Infinite loop
                )
        except Exception as e:
            logger.error(f"Failed to create APNG: {e}")
            raise

        return apng_path
</file>

<file path="src/brosh/models.py">
#!/usr/bin/env python3
# this_file: src/brosh/models.py

"""Data models and enums for the brosh package."""

from enum import Enum
from pathlib import Path
from typing import Any, Literal, Union

from platformdirs import user_pictures_dir
from pydantic import BaseModel, Field
from pydantic.networks import AnyUrl


class ImageFormat(str, Enum):
    """Supported image output formats."""

    PNG = "png"
    JPG = "jpg"
    APNG = "apng"

    @property
    def mime_type(self) -> str:
        """Get the MIME type for this image format."""
        mime_types = {
            self.PNG: "image/png",
            self.JPG: "image/jpeg",
            self.APNG: "image/apng",
        }
        return mime_types[self]

    @property
    def file_extension(self) -> str:
        """Get the file extension for this image format."""
        extensions = {
            self.PNG: ".png",
            self.JPG: ".jpg",
            self.APNG: ".apng",
        }
        return extensions[self]

    @classmethod
    def from_mime_type(cls, mime_type: str) -> "ImageFormat":
        """Create an ImageFormat from a MIME type."""
        mime_map = {
            "image/png": cls.PNG,
            "image/jpeg": cls.JPG,
            "image/jpg": cls.JPG,
            "image/apng": cls.APNG,
        }
        if mime_type not in mime_map:
            msg = f"Unsupported MIME type: {mime_type}"
            raise ValueError(msg)
        return mime_map[mime_type]

    @classmethod
    def from_extension(cls, extension: str) -> "ImageFormat":
        """Create an ImageFormat from a file extension."""
        if not extension.startswith("."):
            extension = f".{extension}"
        ext_map = {
            ".png": cls.PNG,
            ".jpg": cls.JPG,
            ".jpeg": cls.JPG,
            ".apng": cls.APNG,
        }
        if extension.lower() not in ext_map:
            msg = f"Unsupported file extension: {extension}"
            raise ValueError(msg)
        return ext_map[extension.lower()]


class MCPResource(BaseModel):
    """Model for MCP resource content."""

    uri: str = Field(..., description="Resource URI")
    mime_type: str = Field(..., description="MIME type of the resource")
    text: str | None = Field(
        None,
        description="Text content if available",
    )
    blob: str | None = Field(
        None,
        description="Base64-encoded binary data",
    )


class MCPTextContent(BaseModel):
    """Model for MCP text content items."""

    type: Literal["text"] = Field(default="text")
    text: str = Field(..., description="Text content")

    def model_dump(self, **kwargs) -> dict[str, Any]:
        """Override to ensure exclude_none is always True."""
        kwargs["exclude_none"] = True
        return super().model_dump(**kwargs)


class MCPImageContent(BaseModel):
    """Model for MCP image content items."""

    type: Literal["image"] = Field(default="image")
    data: str = Field(..., description="Base64-encoded image data")
    mime_type: str = Field(..., description="MIME type for image content", serialization_alias="mimeType")

    def model_dump(self, **kwargs) -> dict[str, Any]:
        """Override to ensure exclude_none is always True and use by_alias."""
        kwargs["exclude_none"] = True
        kwargs["by_alias"] = True
        return super().model_dump(**kwargs)


class MCPContentItem(BaseModel):
    """Model for MCP content items."""

    type: str = Field(
        ...,
        description="Content type (text, image, resource)",
    )
    text: str | None = Field(
        None,
        description="Text content if type is text",
    )
    data: str | None = Field(
        None,
        description="Base64-encoded data for binary content",
    )
    mime_type: str | None = Field(
        None,
        description="MIME type for binary content",
    )
    resource: MCPResource | None = Field(
        None,
        description="Resource content",
    )

    def to_camel_dict(self) -> dict[str, Any]:
        """Return a dict with camelCase keys for MCP output."""
        d = self.dict(exclude_none=True)
        if "mime_type" in d:
            d["mimeType"] = d.pop("mime_type")
        return d


class MCPScreenshotResult(BaseModel):
    """Model for MCP screenshot result metadata."""

    image: MCPImageContent = Field(..., description="Screenshot image data")
    selector: str = Field(
        "body",
        description="CSS selector for visible element",
    )
    text: str | None = Field(
        None,
        description="Extracted text content",
    )
    html: str | None = Field(
        None,
        description="Extracted HTML content",
    )

    def metadata_json(self, path: str) -> str:
        """Return JSON metadata for the text content item, keyed by file path."""
        import json

        meta = {
            path: {
                "selector": self.selector,
                "text": self.text,
            }
        }
        if self.html is not None:
            meta[path]["html"] = self.html
        return json.dumps(meta, ensure_ascii=False)


class MCPToolResult(BaseModel):
    """Model for MCP tool results."""

    content: list[MCPTextContent | MCPImageContent] = Field(
        ...,
        description="Content items in the result",
    )

    def model_dump(self, **kwargs) -> dict[str, Any]:
        """Override to ensure proper serialization of content items."""
        kwargs["exclude_none"] = True
        # First get the raw data without dumping the content items
        data = super().model_dump(**kwargs, mode="python")
        # Now manually serialize each content item with its own model_dump method
        if "content" in data and self.content:
            serialized_content = []
            for item in self.content:
                # Each item should use its own model_dump method
                serialized_content.append(item.model_dump())
            data["content"] = serialized_content
        return data
</file>

<file path=".cursorindexingignore">
# Don't index SpecStory auto-save files, but allow explicit context inclusion via @ references
.specstory/**
</file>

<file path=".gitignore">
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private
VERSION.txt
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- Initial public release of brosh (Browser Screenshot Tool)
- Playwright-based async implementation for capturing scrolling screenshots
- Support for Chrome, Edge, and Safari browsers
- Smart section detection for descriptive filenames
- Multiple output formats: PNG, JPG, and animated PNG (APNG)
- Remote debugging mode to connect to existing browser sessions
- MCP (Model Context Protocol) server integration
- HTML extraction feature to capture visible element content
- Automatic text extraction: converts visible HTML to Markdown format
- Configurable scroll steps and starting positions
- Automatic browser detection and fallback logic
- Comprehensive error handling and retry mechanisms

### Changed
- Refactored monolithic script into modular Python package structure
- Migrated from script-based to package-based distribution
- Updated to use modern Python packaging with pyproject.toml

### Fixed
- Browser connection issues with improved retry logic
- MCP response format now properly excludes null fields and uses camelCase for field names (e.g., `mimeType` instead of `mime_type`)
- MCP server results now handle size limits properly with progressive compression

### Added
- PNG optimization using pyoxipng for all captured screenshots
- HTML content compression that removes SVG elements while preserving dimensions
- Progressive compression strategy for MCP results exceeding 1MB size limit
- Automatic downsampling and content reduction for oversized MCP responses
- Screenshot timeout handling
- Image scaling and format conversion edge cases

## [0.1.0] - 2025-06-12

### Added
- Initial implementation as a monolithic script
- Basic screenshot capture functionality
- Browser management commands (run, quit)
- Fire-based CLI interface

[Unreleased]: https://github.com/twardoch/brosh/compare/v0.1.0...HEAD
[0.1.0]: https://github.com/twardoch/brosh/releases/tag/v0.1.0
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="package.toml">
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows
</file>

<file path="pyrightconfig.json">
{
  "include": [
    "**/*.py"
  ],
  "exclude": [
    "src",
    "**/node_modules",
    "**/__pycache__"
  ],
  "reportMissingImports": false,
  "reportMissingTypeStubs": false,
  "pythonVersion": "3.10"
}
</file>

<file path="src/brosh/browser.py">
#!/usr/bin/env python3
# this_file: src/brosh/browser.py

"""Browser management utilities for brosh."""

import asyncio
import os
import platform
import subprocess

from loguru import logger
from playwright.async_api import async_playwright


class BrowserManager:
    """Manages browser detection, launching, and connection."""

    def __init__(self, connection_timeout: int = 30):
        """Initialize browser manager.

        Args:
            connection_timeout: Timeout for browser connections in seconds
        """
        self.connection_timeout = connection_timeout
        self.debug_ports = {
            "chromium": 9222,
            "msedge": 9223,
            "webkit": 9225,
        }

    def get_screen_dimensions(self) -> tuple[int, int]:
        """Get main screen dimensions in logical pixels for browser sizing.

        Returns:
            Tuple of (width, height) in logical pixels (CSS pixels)

        """
        if platform.system() == "Darwin":  # macOS
            try:
                # Get physical resolution
                result = subprocess.run(
                    ["system_profiler", "SPDisplaysDataType"],
                    capture_output=True,
                    text=True,
                    check=True,
                    timeout=10,
                )
                for line in result.stdout.split("\n"):
                    if "Resolution:" in line:
                        parts = line.split()
                        for i, part in enumerate(parts):
                            if "x" in part and i > 0:
                                physical_width = int(parts[i - 1])
                                physical_height = int(parts[i + 1])

                                # Check if it's a Retina display
                                if "Retina" in line or physical_width >= 2560:
                                    # Retina: logical = physical / 2
                                    return (
                                        physical_width // 2,
                                        physical_height // 2,
                                    )
                                # Non-Retina: logical = physical
                                return physical_width, physical_height
                        break

            except (
                subprocess.CalledProcessError,
                ValueError,
                IndexError,
                subprocess.TimeoutExpired,
            ) as e:
                logger.warning(f"Failed to get macOS screen dimensions: {e}")

        elif platform.system() == "Windows":
            try:
                import tkinter as tk

                root = tk.Tk()
                # Get logical size (accounts for DPI scaling automatically)
                width = root.winfo_screenwidth()
                height = root.winfo_screenheight()
                root.destroy()
                return width, height
            except ImportError:
                logger.warning("tkinter not available on Windows")

        # Default fallback for unknown systems or errors
        return 1440, 900  # Common laptop logical resolution

    def get_browser_name(self, app: str = "") -> str:
        """Determine browser name from app parameter or OS default.

        Priority order: Chrome > Edge > Safari (macOS only)
        Firefox support removed per user request.

        Args:
            app: User-specified browser preference

        Returns:
            Browser name compatible with Playwright

        """
        if bool(app):
            app_lower = app.lower()
            if "chrome" in app_lower:
                return "chromium"
            if "edge" in app_lower:
                return "msedge"
            if "safari" in app_lower and platform.system() == "Darwin":
                return "webkit"

        # Auto-detect available browser in priority order
        if platform.system() == "Darwin":  # macOS
            # Priority: Chrome > Edge > Safari
            for browser in ["chromium", "msedge", "webkit"]:
                if self.is_browser_available(browser):
                    return browser
        else:  # Windows/Linux
            # Priority: Chrome > Edge
            for browser in ["chromium", "msedge"]:
                if self.is_browser_available(browser):
                    return browser

        # Fallback
        return "chromium"

    def is_browser_available(self, browser_name: str) -> bool:
        """Check if browser is installed and available.

        Args:
            browser_name: Browser name to check

        Returns:
            True if browser is available

        """
        paths = self.get_browser_paths(browser_name)

        # Check if any path exists
        return any(os.path.exists(path) for path in paths)

    def get_browser_paths(self, browser_name: str) -> list:
        """Get possible paths for a browser.

        Args:
            browser_name: Browser name

        Returns:
            List of possible paths
        """
        if browser_name == "chromium":
            return [
                "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
                "/Applications/Chromium.app/Contents/MacOS/Chromium",
                "/usr/bin/google-chrome",
                "/usr/bin/chromium-browser",
                "/opt/google/chrome/chrome",
                ("C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"),
                ("C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe"),
            ]
        if browser_name == "msedge":
            return [
                ("/Applications/Microsoft Edge.app/Contents/MacOS/Microsoft Edge"),
                ("C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe"),
                ("C:\\Program Files\\Microsoft\\Edge\\Application\\msedge.exe"),
            ]
        if browser_name == "webkit":
            return ["/Applications/Safari.app/Contents/MacOS/Safari"]
        return []

    def find_browser_path(self, browser_name: str) -> str | None:
        """Find the path to the specified browser executable.

        Args:
            browser_name: Name of the browser to find

        Returns:
            Path to browser executable or None if not found

        """
        paths = self.get_browser_paths(browser_name)

        for path in paths:
            if os.path.exists(path):
                return path
        return None

    async def get_browser_instance(self, playwright, browser_name: str, width: int, height: int, zoom: int) -> tuple:
        """Get browser instance, connecting to user's actual browser.

        This method tries to connect to the user's existing browser in
        debug mode. If that fails, it will attempt to restart the browser
        in debug mode.

        Args:
            playwright: Playwright instance
            browser_name: Name of browser to use
            width: Viewport width
            height: Viewport height
            zoom: Zoom level percentage

        Returns:
            Tuple of (browser, context, page)

        Raises:
            RuntimeError: If browser connection fails

        """
        debug_port = self.debug_ports.get(browser_name, 9222)

        # Try to connect to existing browser instance first
        browser = None
        try:
            if browser_name in ["chromium", "msedge"]:
                browser = await playwright.chromium.connect_over_cdp(
                    f"http://localhost:{debug_port}",
                    timeout=self.connection_timeout * 1000,
                )

            if browser:
                # Don't set device_scale_factor - let browser use natural scaling
                # Use default height if height is -1 (capture entire page)
                viewport_height = height if height != -1 else 900
                context = await browser.new_context(viewport={"width": width, "height": viewport_height})
                page = await context.new_page()

                # Apply zoom via CSS instead of device scale factor
                if zoom != 100:
                    await page.add_init_script(f"""
                        document.addEventListener('DOMContentLoaded', () => {{
                            document.body.style.zoom = '{zoom}%';
                        }});
                    """)

                return browser, context, page
        except Exception as e:
            logger.info(f"Could not connect to existing browser: {e}")
            logger.info("Attempting to start browser in debug mode...")

        # If we can't connect, try to launch the user's actual browser
        # in debug mode (not Playwright's browser)
        browser = None

        if browser_name == "chromium":
            # Try to launch user's Chrome in debug mode
            chrome_paths = self.get_browser_paths("chromium")

            for chrome_path in chrome_paths:
                if await self.launch_browser_and_connect(
                    chrome_path,
                    debug_port,
                    width,
                    height,
                    playwright.chromium,
                    "chromium",
                ):
                    browser = await playwright.chromium.connect_over_cdp(f"http://localhost:{debug_port}")
                    break

        elif browser_name == "msedge":
            # Try to launch user's Edge in debug mode
            edge_paths = self.get_browser_paths("msedge")

            for edge_path in edge_paths:
                if await self.launch_browser_and_connect(
                    edge_path,
                    debug_port,
                    width,
                    height,
                    playwright.chromium,
                    "msedge",
                ):
                    browser = await playwright.chromium.connect_over_cdp(f"http://localhost:{debug_port}")
                    break

        elif browser_name == "webkit":
            # For Safari, we need to enable "Develop" menu first
            logger.info("For Safari: Enable Develop menu in Preferences > Advanced")
            logger.info("Then enable 'Allow Remote Automation' in Develop menu")
            # Safari doesn't support remote debugging like Chrome/Firefox
            # Fall back to launching webkit
            browser = await playwright.webkit.launch(headless=False)

        if not browser:
            msg = (
                f"Could not connect to or launch {browser_name} browser. "
                "Please ensure the browser is installed and try again."
            )
            raise RuntimeError(msg)

        # Create context without device scale factor to avoid scaling issues
        # Use default height if height is -1 (capture entire page)
        viewport_height = height if height != -1 else 900
        context = await browser.new_context(viewport={"width": width, "height": viewport_height})
        page = await context.new_page()

        # Apply zoom via CSS instead of device scale factor
        if zoom != 100:
            await page.add_init_script(f"""
                document.addEventListener('DOMContentLoaded', () => {{
                    document.body.style.zoom = '{zoom}%';
                }});
            """)

        return browser, context, page

    async def launch_browser_and_connect(
        self,
        browser_path: str,
        debug_port: int,
        width: int,
        height: int,
        playwright_browser,
        browser_type: str,
    ) -> bool:
        """Launch browser with debug mode and test connection.

        Args:
            browser_path: Path to browser executable
            debug_port: Debug port to use
            width: Window width
            height: Window height
            playwright_browser: Playwright browser module
            browser_type: Type of browser (chromium, msedge)

        Returns:
            True if successfully launched and connected

        """
        if not os.path.exists(browser_path):
            logger.debug(f"Browser path does not exist: {browser_path}")
            return False

        try:
            # Kill existing processes with same debug port - more aggressive cleanup
            try:
                if platform.system() == "Darwin":  # macOS
                    # Kill by process name and port
                    subprocess.run(
                        ["pkill", "-f", f"remote-debugging-port={debug_port}"],
                        capture_output=True,
                        timeout=5,
                        check=False,
                    )
                    # Also try killing by process name
                    if "Chrome" in browser_path:
                        subprocess.run(
                            ["pkill", "-f", "Google Chrome.*remote-debugging"],
                            capture_output=True,
                            timeout=5,
                            check=False,
                        )
                else:  # Windows/Linux
                    subprocess.run(
                        ["taskkill", "/F", "/IM", "chrome.exe"],
                        capture_output=True,
                        timeout=5,
                        check=False,
                    )
            except Exception as e:
                logger.debug(f"Process cleanup warning: {e}")

            await asyncio.sleep(2)  # Give processes time to die

            # Launch browser with remote debugging
            if browser_type in ["chromium", "msedge"]:
                args = [
                    browser_path,
                    f"--remote-debugging-port={debug_port}",
                    "--no-startup-window",
                    "--noerrdialogs",
                    "--no-user-gesture-required",
                    "--no-network-profile-warning",
                    "--no-first-run",
                    "--no-experiments",
                    "--no-default-browser-check",
                    "--remote-debug-mode",
                    "--disable-web-security",
                    "--disable-features=VizDisplayCompositor",
                    "--disable-background-timer-throttling",
                    "--disable-backgrounding-occluded-windows",
                    "--disable-renderer-backgrounding",
                    "--disable-infobars",
                    "--disable-extensions",
                    "--disable-sync",
                    "--disable-translate",
                    "--disable-background-networking",
                    f"--window-size={width},{height}",
                    "--user-data-dir=/tmp/chrome-debug-brosh",
                ]
            else:
                return False

            logger.info(f"Launching {browser_type} with debug port {debug_port}")
            process = subprocess.Popen(args, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

            # Wait for browser to start and test connection more robustly
            for attempt in range(10):  # More attempts
                await asyncio.sleep(1)  # Shorter intervals
                try:
                    if browser_type in ["chromium", "msedge"]:
                        test_browser = await playwright_browser.connect_over_cdp(
                            f"http://localhost:{debug_port}", timeout=5000
                        )
                    else:
                        return False

                    # Test that we can actually create a page
                    test_context = await test_browser.new_context()
                    test_page = await test_context.new_page()
                    await test_page.close()
                    await test_context.close()
                    await test_browser.close()

                    logger.info(f"Successfully launched {browser_type} at {browser_path}")
                    return True

                except Exception as e:
                    logger.debug(f"Connection attempt {attempt + 1}/10 failed: {e}")
                    if attempt == 9:  # Last attempt
                        # Kill the process we started if it's still running
                        try:
                            process.terminate()
                            await asyncio.sleep(1)
                            if process.poll() is None:
                                process.kill()
                        except Exception:
                            pass
                        return False
                    continue

        except Exception as e:
            logger.error(f"Failed to launch {browser_type} at {browser_path}: {e}")
            return False

        return False  # Explicit return for all paths

    async def cleanup_browser(self, page, context, browser) -> None:
        """Clean up browser resources safely.

        Args:
            page: Playwright page instance
            context: Playwright context instance
            browser: Playwright browser instance

        """
        try:
            if page:
                await page.close()
        except Exception as e:
            logger.warning(f"Failed to close page: {e}")

        try:
            if context:
                await context.close()
        except Exception as e:
            logger.warning(f"Failed to close context: {e}")

        try:
            if hasattr(browser, "_browser") and browser._browser:
                await browser.close()
        except Exception as e:
            logger.warning(f"Failed to close browser: {e}")

    def get_browser_args(self, browser_type: str, width: int, height: int, debug_port: int) -> list:
        """Get browser launch arguments.

        Args:
            browser_type: Type of browser
            width: Window width
            height: Window height
            debug_port: Debug port

        Returns:
            List of command line arguments
        """
        if browser_type in ["chromium", "msedge"]:
            return [
                f"--remote-debugging-port={debug_port}",
                "--no-startup-window",
                "--noerrdialogs",
                "--no-user-gesture-required",
                "--no-network-profile-warning",
                "--no-first-run",
                "--no-experiments",
                "--no-default-browser-check",
                "--disable-web-security",
                "--disable-features=VizDisplayCompositor",
                "--disable-background-timer-throttling",
                "--disable-backgrounding-occluded-windows",
                "--disable-renderer-backgrounding",
                "--disable-infobars",
                "--disable-extensions",
                "--disable-sync",
                "--disable-translate",
                "--disable-background-networking",
                f"--window-size={width},{height}",
                "--user-data-dir=/tmp/chrome-debug-brosh",
            ]
        return []
</file>

<file path="src/brosh/capture.py">
#!/usr/bin/env python3
# this_file: src/brosh/capture.py

"""Screenshot capture logic for brosh."""

import asyncio
from datetime import datetime
from pathlib import Path

from loguru import logger
from playwright.async_api import TimeoutError as PlaywrightTimeoutError

from .image import ImageProcessor


class CaptureManager:
    """Manages screenshot capture operations."""

    def __init__(self, page_timeout: int = 60, screenshot_timeout: int = 10):
        """Initialize capture manager.

        Args:
            page_timeout: Page load timeout in seconds
            screenshot_timeout: Screenshot capture timeout in seconds
        """
        self.page_timeout = page_timeout
        self.screenshot_timeout = screenshot_timeout
        self.image_processor = ImageProcessor()

    def validate_inputs(self, url: str, zoom: int, scroll_step: int, scale: int, format: str) -> None:
        """Validate input parameters.

        Args:
            url: URL to validate
            zoom: Zoom level to validate
            scroll_step: Scroll step to validate
            scale: Scale to validate
            format: Format to validate

        Raises:
            ValueError: For invalid parameters

        """
        if not url or not url.startswith(("http://", "https://")):
            msg = f"Invalid URL: {url}"
            raise ValueError(msg)

        if not (10 <= zoom <= 500):
            msg = f"Zoom must be between 10-500%: {zoom}"
            raise ValueError(msg)

        if not (10 <= scroll_step <= 200):
            msg = f"Scroll step must be between 10-200%: {scroll_step}"
            raise ValueError(msg)

        if not (10 <= scale <= 200):
            msg = f"Scale must be between 10-200%: {scale}"
            raise ValueError(msg)

        if format.lower() not in ["png", "jpg", "apng"]:
            msg = f"Unsupported format: {format}. Use: png, jpg, apng"
            raise ValueError(msg)

    async def capture_screenshots(
        self,
        page,
        url: str,
        domain: str,
        output_path: Path,
        width: int,
        height: int,
        scroll_step: int,
        scale: int,
        img_format: str,
        anim_spf: float,
        temp_png_paths: list[Path],
        html: bool = False,
        max_frames: int = 0,
        from_selector: str = "",
    ) -> tuple[list[str], dict[str, str]]:
        """Capture all screenshots for the page.

        Args:
            page: Playwright page instance
            url: URL being captured
            domain: Domain name for filenames
            output_path: Output directory
            width: Viewport width
            height: Viewport height
            scroll_step: Scroll step percentage
            scale: Image scale percentage
            img_format: Output format
            anim_spf: Animation seconds per frame
            temp_png_paths: List to store temp PNG paths
            html: Whether to capture HTML/selectors
            max_frames: Maximum number of frames to capture
            from_selector: CSS selector to scroll to before starting

        Returns:
            Tuple of (saved file paths, html data dict)

        """
        saved_paths = []
        html_data = {}

        # Navigate to URL with timeout and retries
        try:
            logger.info(f"Navigating to {url}")
            await page.goto(
                url,
                wait_until="domcontentloaded",
                timeout=self.page_timeout * 1000,
            )
            await asyncio.sleep(3)  # Additional wait for dynamic content
        except PlaywrightTimeoutError:
            logger.warning("Page load timeout, proceeding anyway")
        except Exception as e:
            msg = f"Failed to navigate to {url}: {e}"
            raise RuntimeError(msg)

        # Handle from_selector - scroll to element before starting
        start_position = 0
        if from_selector:
            try:
                logger.info(f"Scrolling to element: {from_selector}")
                # Scroll element into view and get its position
                start_position = await page.evaluate(f"""
                    (() => {{
                        const element = document.querySelector('{from_selector}');
                        if (element) {{
                            element.scrollIntoView({{behavior: 'instant', block: 'start'}});
                            return element.getBoundingClientRect().top + window.pageYOffset;
                        }}
                        return 0;
                    }})()
                """)
                await asyncio.sleep(1)  # Wait for scroll to complete
                logger.info(f"Starting capture from position: {start_position}px")
            except Exception as e:
                logger.warning(f"Failed to find selector '{from_selector}': {e}, starting from top")
                start_position = 0

        # Get total page height for scroll calculation
        try:
            total_height = await page.evaluate("document.documentElement.scrollHeight")
            # Handle height == -1 to capture entire page
            if height == -1:
                viewport_height = await page.evaluate("window.innerHeight")
                logger.info(f"Capturing entire page - height: {total_height}px, viewport: {viewport_height}px")
            else:
                viewport_height = height
                logger.info(f"Page height: {total_height}px, viewport: {viewport_height}px")
        except Exception as e:
            msg = f"Failed to get page dimensions: {e}"
            raise RuntimeError(msg)

        # Calculate all scroll positions based on step size
        scroll_positions = []
        current_pos = start_position
        while current_pos < total_height:
            scroll_positions.append(int(current_pos))
            current_pos += int(viewport_height * scroll_step / 100)

        # Limit frames if max_frames is specified
        if max_frames > 0:
            scroll_positions = scroll_positions[:max_frames]

        logger.info(f"Will capture {len(scroll_positions)} screenshots")

        # Generate timestamp for filename
        now = datetime.now()
        timestamp = now.strftime("%y%m%d-%H%M%S")

        # Capture screenshots at each scroll position
        for _i, scroll_pos in enumerate(scroll_positions):
            try:
                # Scroll to the calculated position
                await page.evaluate(f"window.scrollTo(0, {scroll_pos})")
                await asyncio.sleep(0.8)  # Wait for scroll animation and content load

                # Calculate scroll percentage for filename
                scroll_percentage = min(int((scroll_pos / total_height) * 10000), 9999)

                # Get semantic section ID based on visible content
                section_id = await self.get_section_id(page)

                # Generate descriptive filename
                if img_format == "apng":
                    # For APNG, save as PNG first, convert later
                    filename = f"{domain}-{timestamp}-{scroll_percentage:05d}-{section_id}.png"
                    filepath = output_path / filename
                    temp_png_paths.append(filepath)
                else:
                    filename = f"{domain}-{timestamp}-{scroll_percentage:05d}-{section_id}.{img_format}"
                    filepath = output_path / filename

                # Capture the visible area screenshot with timeout
                try:
                    await page.screenshot(
                        path=str(filepath),
                        full_page=False,
                        timeout=self.screenshot_timeout * 1000,
                    )
                except PlaywrightTimeoutError:
                    logger.warning(f"Screenshot timeout for position {scroll_pos}, skipping")
                    continue

                # Apply scaling if requested
                if scale != 100:
                    self.image_processor.scale_image(filepath, scale)

                # Convert to JPG if needed
                if img_format == "jpg":
                    filepath = self.image_processor.convert_to_jpg(filepath)
                elif img_format == "png":
                    # Optimize PNG files
                    self.image_processor.optimize_png(filepath)

                if img_format != "apng":
                    saved_paths.append(str(filepath))
                    logger.debug(f"Captured: {filepath}")

                # Always capture HTML and convert to text
                selector = await self.get_visible_selector(page)
                visible_html = await self.get_visible_html(page)

                # Convert HTML to text
                import html2text

                h = html2text.HTML2Text()
                h.ignore_links = False
                h.ignore_images = True
                h.body_width = 0  # Don't wrap lines
                visible_text = h.handle(visible_html).strip()

                # Store data based on html flag
                if html:
                    # Include HTML when requested
                    html_data[str(filepath)] = {"selector": selector, "html": visible_html, "text": visible_text}
                else:
                    # Only include selector and text
                    html_data[str(filepath)] = {"selector": selector, "text": visible_text}

            except Exception as e:
                logger.error(f"Failed to capture screenshot at position {scroll_pos}: {e}")
                continue  # Continue with next screenshot

        # Create APNG if requested
        if img_format == "apng" and temp_png_paths:
            try:
                apng_path = self.image_processor.create_apng(temp_png_paths, domain, output_path, anim_spf)
                saved_paths.append(str(apng_path))
                logger.info(f"Created APNG: {apng_path}")

                # Clean up temporary PNG files
                for temp_path in temp_png_paths:
                    try:
                        temp_path.unlink()
                    except Exception as e:
                        logger.warning(f"Failed to delete temp file {temp_path}: {e}")
            except Exception as e:
                logger.error(f"Failed to create APNG: {e}")

        return saved_paths, html_data

    async def get_section_id(self, page) -> str:
        """Get a smart ID based on current visible content.

        This method attempts to identify the current section by looking
        for visible headers or elements with IDs in the viewport.

        Args:
            page: Playwright page instance

        Returns:
            Section identifier string

        """
        try:
            # Execute JavaScript to find visible headers in viewport
            headers = await page.evaluate("""() => {
                const viewportHeight = window.innerHeight;
                const headers = Array.from(
                    document.querySelectorAll('h1, h2, h3, h4, h5, h6, [id]')
                );

                for (const header of headers) {
                    const rect = header.getBoundingClientRect();
                    if (rect.top >= 0 && rect.top < viewportHeight / 2) {
                        return (header.id || header.textContent || '').trim()
                            .toLowerCase()
                            .replace(/[^a-z0-9]+/g, '-')
                            .replace(/^-+|-+$/g, '')
                            .substring(0, 20);
                    }
                }
                return 'section';
            }""")

            return headers or "section"
        except Exception:
            return "section"

    async def get_visible_html(self, page) -> str:
        """Get minified HTML of visible portion of the page.

        Args:
            page: Playwright page instance

        Returns:
            Minified HTML string of visible elements

        """
        try:
            return await page.evaluate("""() => {
                const {innerHeight: H, innerWidth: W} = window;
                const nodes = [...document.querySelectorAll('*')];
                const fullyVisibleElements = [];

                // Tags to exclude from capture
                const excludeTags = ['HTML', 'HEAD', 'BODY', 'SCRIPT', 'STYLE', 'META', 'LINK', 'TITLE'];

                // Find elements that are FULLY visible in the viewport
                nodes.forEach(node => {
                    // Skip excluded tags and non-element nodes
                    if (excludeTags.includes(node.tagName) || node.nodeType !== 1) {
                        return;
                    }

                    const r = node.getBoundingClientRect();
                    // Check if element is fully visible
                    if (r.top >= 0 && r.bottom <= H && r.left >= 0 && r.right <= W && r.width > 0 && r.height > 0) {
                        // Check if this element is already contained in a parent we've added
                        let isContained = false;
                        for (const existing of fullyVisibleElements) {
                            if (existing.contains(node)) {
                                isContained = true;
                                break;
                            }
                        }
                        if (!isContained) {
                            // Remove any previously added children of this element
                            const filtered = fullyVisibleElements.filter(el => !node.contains(el));
                            fullyVisibleElements.length = 0;
                            fullyVisibleElements.push(...filtered, node);
                        }
                    }
                });

                // Convert to HTML strings and concatenate
                const htmlParts = fullyVisibleElements.map(el => el.outerHTML);

                // Return minified HTML
                return htmlParts.join('').replace(/\\s+/g, ' ').trim();
            }""")
        except Exception as e:
            logger.error(f"Failed to get visible HTML: {e}")
            return ""

    async def get_visible_selector(self, page) -> str:
        """Get a good selector for the visible portion of the page.

        Args:
            page: Playwright page instance

        Returns:
            CSS selector string for visible portion

        """
        try:
            return await page.evaluate("""() => {
                const {innerHeight: H} = window;

                // Try to find the most specific container for visible content
                const candidates = [
                    'main', 'article', '[role="main"]', '.content', '#content',
                    'section:first-of-type', 'div.container'
                ];

                for (const sel of candidates) {
                    const el = document.querySelector(sel);
                    if (el) {
                        const r = el.getBoundingClientRect();
                        if (r.top < H && r.bottom > 0) {
                            return sel;
                        }
                    }
                }

                // Find first visible section or div
                const sections = [...document.querySelectorAll('section, div')];
                for (const section of sections) {
                    const r = section.getBoundingClientRect();
                    if (r.top >= 0 && r.top < H/2) {
                        if (section.id) return '#' + section.id;
                        if (section.className) {
                            const classes = section.className.split(' ').filter(c => c);
                            if (classes.length) return '.' + classes.join('.');
                        }
                    }
                }

                // Fallback to body viewport
                return 'body';
            }""")
        except Exception as e:
            logger.error(f"Failed to get visible selector: {e}")
            return "body"
</file>

<file path="src/brosh/cli.py">
#!/usr/bin/env python3
# this_file: src/brosh/cli.py

"""CLI interface for brosh."""

import asyncio
import platform
import subprocess
import time
from pathlib import Path

from loguru import logger
from platformdirs import user_pictures_dir

from .browser import BrowserManager
from .models import (
    ImageFormat,
    MCPImageContent,
    MCPScreenshotResult,
    MCPTextContent,
    MCPToolResult,
    ScreenshotRequest,
)
from .tool import BrowserScreenshotTool


class BrowserScreenshotCLI:
    """Fire CLI interface for browser screenshot operations.

    Provides organized commands for browser management and screenshot capture.

    """

    def __init__(
        self,
        app: str = "",
        width: int = 0,
        height: int = 0,
        zoom: int = 100,
        output_dir: Path = Path(user_pictures_dir()),
        subdirs: bool = False,
        verbose: bool = False,
        json: bool = False,
    ) -> None:
        """Initialize CLI with common parameters.

        Args:
            app: Browser to use - chrome, edge, safari (default: auto-detect)
            width: Width in pixels (default: screen width)
            height: Height in pixels (-1: no limit, default: screen height)
            zoom: Zoom level in % (default: 100)
            output_dir: Output folder for screenshots (default: user's pictures)
            subdirs: Create subfolders per domain
            verbose: Enable debug logging

        """
        self.app = app
        self.width = width
        self.height = height
        self.zoom = zoom
        self.output_dir = output_dir
        self.subdirs = subdirs
        self.json = json
        self.verbose = verbose
        self._tool = BrowserScreenshotTool(verbose=verbose)
        self._browser_manager = BrowserManager()

    def run(self, force_run: bool = False) -> str:
        """Run browser in remote debug mode.

        Args:
            force_run: Always restart browser even if already running

        Returns:
            Status message

        """
        browser_name = self._browser_manager.get_browser_name(self.app)
        debug_ports = self._browser_manager.debug_ports
        debug_port = debug_ports.get(browser_name, 9222)

        # Check if already running
        if not force_run:
            try:
                import urllib.request

                urllib.request.urlopen(f"http://localhost:{debug_port}/json", timeout=2)
                return f"{browser_name} already running on port {debug_port}"
            except Exception:
                pass

        # Kill existing processes first if force_run
        if force_run:
            self.quit()
            time.sleep(2)

        # Launch browser directly with debug args
        browser_path = self._browser_manager.find_browser_path(browser_name)
        if not browser_path:
            return f"Could not find {browser_name} installation"

        try:
            width = self.width or 1440
            height = self.height or 900

            args = [browser_path, *self._browser_manager.get_browser_args(browser_name, width, height, debug_port)]

            if not args[1:]:  # No args returned (not chromium/msedge)
                return f"Browser {browser_name} not supported for direct launch"

            logger.info(f"Starting {browser_name} with debug port {debug_port}")
            subprocess.Popen(args, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

            # Wait and verify connection
            for _attempt in range(10):
                time.sleep(1)
                try:
                    import urllib.request

                    urllib.request.urlopen(f"http://localhost:{debug_port}/json", timeout=2)
                    return f"Started {browser_name} in debug mode on port {debug_port}"
                except Exception:
                    continue

            return f"Started {browser_name} but could not verify debug connection"

        except Exception as e:
            return f"Failed to start {browser_name}: {e}"

    def quit(self) -> str:
        """Quit the specified browser.

        Returns:
            Status message

        """
        browser_name = self._browser_manager.get_browser_name(self.app)
        debug_ports = self._browser_manager.debug_ports
        debug_port = debug_ports.get(browser_name, 9222)

        try:
            if platform.system() == "Darwin":  # macOS
                subprocess.run(
                    ["pkill", "-f", f"remote-debugging-port={debug_port}"],
                    capture_output=True,
                    timeout=5,
                    check=False,
                )
                if "chrome" in browser_name.lower():
                    subprocess.run(
                        ["pkill", "-f", "Google Chrome.*remote-debugging"],
                        capture_output=True,
                        timeout=5,
                        check=False,
                    )
            else:  # Windows/Linux
                subprocess.run(
                    ["taskkill", "/F", "/IM", "chrome.exe"],
                    capture_output=True,
                    timeout=5,
                    check=False,
                )

            return f"Quit {browser_name}"
        except Exception as e:
            return f"Failed to quit {browser_name}: {e}"

    def shot(
        self,
        url: str,
        scroll_step: int = 100,
        scale: int = 100,
        format: str = "png",
        anim_spf: float = 0.5,
        html: bool = False,
        max_frames: int = 0,
        from_selector: str = "",
    ) -> list[str] | dict[str, str] | str:
        """Take screenshots of a webpage.

        Automatically ensures browser is running in debug mode.

        Args:
            url: The URL to navigate to (mandatory)
            scroll_step: Scroll step in % of height (default: 100)
            scale: Scale in % for resampling output image (default: 100)
            format: Output format - png, jpg, or apng (default: png)
            anim_spf: Seconds per frame for APNG animation (default: 0.5)
            html: Return dict with HTML/selectors instead of list (default: False)
            max_frames: Maximum number of frames to capture, 0 for all (default: 0)
            json: Return JSON string output (default: False)
            from_selector: CSS selector to scroll to before starting capture (default: "")

        Returns:
            If json=True: JSON string of the results
            If html=True: Dict with screenshot paths as keys and values containing selector, text, and HTML
            If html=False: Dict with screenshot paths as keys and values containing selector and text
            Legacy: List of paths to saved screenshot files (when no HTML/text extraction)

        """
        # Ensure browser is running in debug mode
        self.run(force_run=False)

        result = asyncio.run(
            self._tool.capture(
                url=url,
                zoom=self.zoom,
                width=self.width,
                height=self.height,
                scroll_step=scroll_step,
                scale=scale,
                app=self.app,
                output_dir=self.output_dir,
                subdirs=self.subdirs,
                mcp=False,
                format=format,
                anim_spf=anim_spf,
                html=html,
                max_frames=max_frames,
                from_selector=from_selector,
            )
        )

        if self.json:
            import json as json_module

            return json_module.dumps(result, indent=2)
        return result

    def mcp(self) -> None:
        """Run MCP server for browser screenshots.

        Automatically ensures browser is running in debug mode.

        """
        # Ensure browser is running in debug mode
        self.run(force_run=False)

        # Import and run MCP server
        from .mcp import run_mcp_server

        run_mcp_server()
</file>

<file path="src/brosh/tool.py">
#!/usr/bin/env python3
# this_file: src/brosh/tool.py

"""Main screenshot tool implementation for brosh."""

import asyncio
import sys
from pathlib import Path
from urllib.parse import urlparse

import platformdirs
from loguru import logger
from playwright.async_api import async_playwright

from .browser import BrowserManager
from .capture import CaptureManager
from .image import ImageProcessor
from .models import ImageFormat


class BrowserScreenshotTool:
    """Tool for capturing scrolling screenshots using Playwright async API.

    Optimized for reliability with comprehensive error handling,
    intelligent browser detection, and performance optimizations.

    """

    def __init__(self, verbose: bool = False):
        """Initialize the screenshot _tool with default settings.

        Args:
            verbose: Enable debug logging

        """
        self.max_retries = 3
        self.connection_timeout = 30
        self.page_timeout = 60
        self.screenshot_timeout = 10
        self.verbose = verbose

        # Configure logging based on verbose flag
        if not verbose:
            logger.remove()
            logger.add(sys.stderr, level="ERROR")

        # Initialize managers
        self.browser_manager = BrowserManager(self.connection_timeout)
        self.capture_manager = CaptureManager(self.page_timeout, self.screenshot_timeout)
        self.image_processor = ImageProcessor()

    async def capture(
        self,
        url: str,
        zoom: int = 100,
        width: int = 0,
        height: int = 0,
        scroll_step: int = 100,
        scale: int = 100,
        app: str = "",
        output_dir: str = platformdirs.user_pictures_dir(),
        subdirs: bool = False,
        mcp: bool = False,
        format: str = "png",
        anim_spf: float = 0.5,
        html: bool = False,
        max_frames: int = 0,
        from_selector: str = "",
    ) -> list[str] | dict[str, str]:
        """Capture screenshots of a webpage using Playwright.

        This method navigates to a URL and captures sequential screenshots
        while scrolling through the page. Each screenshot is named with
        domain, scroll position, and section identifier.

        Args:
            url: The URL to navigate to (mandatory)
            zoom: Zoom level in % (default: 100)
            width: Width in pixels (default: main screen width)
            height: Height in pixels (default: main screen height). Use -1 to capture entire page
            scroll_step: Scroll step in % of height (default: 100)
            scale: Scale in % for resampling output image (default: 100)
            app: Browser to use - chrome, edge, safari (default: auto-detect)
            output_dir: Output directory for screenshots (default: Pictures)
            subdirs: Create subdirectories for domains (default: False)
            mcp: Run in FastMCP mode (default: False)
            format: Output format - png, jpg, or apng (default: png)
            anim_spf: Seconds per frame for APNG animation (default: 0.5)
            html: Return dict with HTML/selectors instead of list
                 (default: False)
            max_frames: Maximum number of frames to capture, 0 for all
                       (default: 0)
            from_selector: CSS selector to scroll to before starting capture
                          (default: "")

        Returns:
            If html=True: Dict with screenshot paths as keys and HTML/selectors
                         as values
            If html=False: List of paths to saved screenshot files

        Raises:
            ValueError: For invalid parameters
            RuntimeError: For browser connection or navigation failures

        """
        if mcp:
            # This should be handled in mcp.py now
            msg = "MCP mode should be handled by mcp module"
            raise RuntimeError(msg)

        # Validate inputs
        self.capture_manager.validate_inputs(url, zoom, scroll_step, scale, format)
        img_format = format.lower()

        # Parse URL and get domain for filename generation
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.replace("www.", "").replace(".", "_")
        if not domain:
            msg = f"Invalid URL: {url}"
            raise ValueError(msg)

        # Create output directory structure
        output_path = Path(output_dir)
        if subdirs:
            output_path = output_path / domain
        output_path.mkdir(parents=True, exist_ok=True)

        # Get screen dimensions if not specified
        if width == 0 or (height == 0 or height == -1):
            default_width, default_height = self.browser_manager.get_screen_dimensions()
            width = width or default_width
            if height == 0:
                height = default_height
            # If height is -1, we'll handle it as "capture entire page"

        if height == -1:
            logger.info(f"Starting capture of {url} at {width}x(entire page)")
        else:
            logger.info(f"Starting capture of {url} at {width}x{height}")

        # Determine browser to use (no Firefox support)
        browser_name = self.browser_manager.get_browser_name(app)
        logger.info(f"Using browser: {browser_name}")

        saved_paths = []
        html_data = {}  # For HTML/selector data when html=True
        temp_png_paths: list[Path] = []  # For APNG conversion

        # Retry mechanism for browser connection
        for attempt in range(self.max_retries):
            try:
                async with async_playwright() as p:
                    # Connect to existing browser or launch new one
                    browser, context, page = await self.browser_manager.get_browser_instance(
                        p, browser_name, width, height, zoom
                    )

                    try:
                        saved_paths, html_data = await self.capture_manager.capture_screenshots(
                            page,
                            url,
                            domain,
                            output_path,
                            width,
                            height,
                            scroll_step,
                            scale,
                            img_format,
                            anim_spf,
                            temp_png_paths,
                            html,
                            max_frames,
                            from_selector,
                        )
                        logger.info(f"Successfully captured {len(saved_paths)} screenshots")
                        break  # Success, exit retry loop

                    finally:
                        # Clean up browser resources
                        await self.browser_manager.cleanup_browser(page, context, browser)

            except Exception as e:
                logger.error(f"Attempt {attempt + 1}/{self.max_retries} failed: {e}")
                if attempt == self.max_retries - 1:
                    msg = f"Failed to capture screenshots after {self.max_retries} attempts: {e}"
                    raise RuntimeError(msg)
                await asyncio.sleep(2)  # Wait before retry

        # Always return html_data when populated (either HTML content or selectors)
        if html_data:
            return html_data
        return saved_paths
</file>

<file path="cleanup.sh">
#!/usr/bin/env bash

rm -rf dist/brosh*.*
uv build

python -m uzpy run -e src
fd -e py -x autoflake {}
fd -e py -x pyupgrade --py311-plus {}
fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}
fd -e py -x ruff format --respect-gitignore --target-version py311 {}
repomix -i varia,.specstory,AGENT.md,CLAUDE.md,PLAN.md,SPEC.md,llms.txt,.cursorrules -o llms.txt .
python -m pytest
</file>

<file path="pyproject.toml">
# this_file: pyproject.toml
#==============================================================================
# BROSH PACKAGE CONFIGURATION
# This pyproject.toml defines the package metadata, dependencies, build system,
# and development environment for the brosh package.
#==============================================================================

#------------------------------------------------------------------------------
# PROJECT METADATA
# Core package information used by PyPI and package managers.
#------------------------------------------------------------------------------
[project]
name = 'brosh' # Package name on PyPI
description = 'Browser screenshot tool using Playwright async API' # Short description
readme = 'README.md' # Path to README file
requires-python = '>=3.10' # Minimum Python version
keywords = [
    'screenshot',
    'browser',
    'playwright',
    'web',
    'capture',
    'automation',
    'mcp',
] # Keywords for PyPI search
dynamic = ["version"] # Fields set dynamically at build time

# PyPI classifiers for package categorization
classifiers = [
    'Development Status :: 4 - Beta', # Package maturity level
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Programming Language :: Python :: Implementation :: PyPy',
    'Operating System :: OS Independent',
    'License :: OSI Approved :: MIT License',
    'Intended Audience :: Developers',
]

dependencies = [
    'fire>=0.5.0',
    'playwright>=1.40.0',
    'pillow>=11.2.1',
    'fastmcp>=2.8.0',
    'platformdirs>=4.0.0',
    'loguru>=0.7.0',
    'html2text>=2025.4.15',
    'pyoxipng>=9.1.1',
]

# Author information
[[project.authors]]
name = 'Adam Twardoch'
email = 'adam+github@twardoch.com'

# License information
[project.license]
text = 'MIT'

# Project URLs
[project.urls]
Documentation = 'https://github.com/twardoch/brosh#readme'
Issues = 'https://github.com/twardoch/brosh/issues'
Source = 'https://github.com/twardoch/brosh'

#------------------------------------------------------------------------------
# OPTIONAL DEPENDENCIES
# Additional dependencies for optional features, development, and testing.
#------------------------------------------------------------------------------
[project.optional-dependencies]

# Development tools
dev = [
    'pre-commit>=4.1.0', # Pre-commit hook manager - Keep pre-commit as is, update if newer pre-commit version is required
    'ruff>=0.9.7', # Linting and formatting - Keep ruff as is, update if newer ruff version is required
    'mypy>=1.15.0', # Type checking - Keep mypy as is, update if newer mypy version is required
    'absolufy-imports>=0.3.1', # Convert relative imports to absolute - Keep absolufy-imports as is, update if newer absolufy-imports version is required
    'pyupgrade>=3.19.1', # Upgrade Python syntax - Keep pyupgrade as is, update if newer pyupgrade version is required
    'isort>=6.0.1', # Sort imports - Keep isort as is, update if newer isort version is required
    'uzpy>=1.0.0', 
]

# Testing tools and frameworks
test = [
    'pytest>=8.3.4', # Testing framework - Keep pytest as is, update if newer pytest version is required
    'pytest-cov>=6.0.0', # Coverage plugin for pytest - Keep pytest-cov as is, update if newer pytest-cov version is required
    'pytest-xdist>=3.6.1', # Parallel test execution - Keep pytest-xdist as is, update if newer pytest-xdist version is required
    'pytest-benchmark[histogram]>=5.1.0', # Benchmarking plugin - Keep pytest-benchmark as is, update if newer pytest-benchmark version is required
    'pytest-asyncio>=1.0.0', # Async test support - Keep pytest-asyncio as is, update if newer pytest-asyncio version is required
    'coverage[toml]>=7.6.12',
]

docs = [
    "sphinx>=8.2.3",
    "sphinx-rtd-theme>=3.0.2",
    "sphinx-autodoc-typehints>=3.2.0",
    "myst-parser>=4.0.1", # Markdown support in Sphinx
]

# All optional dependencies combined
all = [
]

#------------------------------------------------------------------------------
# COMMAND-LINE SCRIPTS
# Entry points for command-line executables installed with the package.
#------------------------------------------------------------------------------
[project.scripts]
brosh = "brosh.__main__:main"
brosh-mcp = "brosh.mcp:main"

#------------------------------------------------------------------------------
# BUILD SYSTEM CONFIGURATION
# Defines the tools required to build the package and the build backend.
#------------------------------------------------------------------------------
[build-system]
# Hatchling is a modern build backend for Python packaging
# hatch-vcs integrates with version control systems for versioning
requires = [
    'hatchling>=1.27.0', # Keep hatchling as is, update if newer hatchling version is required
    'hatch-vcs>=0.4.0', # Keep hatch-vcs as is, update if newer hatch-vcs version is required
]
build-backend = 'hatchling.build' # Specifies Hatchling as the build backend


#------------------------------------------------------------------------------
# HATCH BUILD CONFIGURATION
# Configures the build process, specifying which packages to include and
# how to handle versioning.
#------------------------------------------------------------------------------
[tool.hatch.build]
# Include package data files
include = [
    "src/brosh/py.typed", # For better type checking support
    "src/brosh/data/**/*", # Include data files if any

]
exclude = ["**/__pycache__", "**/.pytest_cache", "**/.mypy_cache"]

[tool.hatch.build.targets.wheel]
packages = ["src/brosh"]
reproducible = true


# Version control system hook configuration
# Automatically updates the version file from git tags
[tool.hatch.build.hooks.vcs]
version-file = "src/brosh/__version__.py"

# Version source configuration
[tool.hatch.version]
source = 'vcs' # Get version from git tags or other VCS info

# Metadata handling configuration
[tool.hatch.metadata]
allow-direct-references = true # Allow direct references in metadata (useful for local dependencies)


#------------------------------------------------------------------------------
# DEVELOPMENT ENVIRONMENTS

[tool.hatch.envs.default]
features = ['dev', 'test', 'all']
dependencies = [
]

# Commands available in the default environment
[tool.hatch.envs.default.scripts]
# Run tests with optional arguments
test = 'pytest {args:tests}'
# Run tests with coverage reporting
test-cov = "pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/brosh --cov=tests {args:tests}"
# Run type checking
type-check = "mypy src/brosh tests"
# Run linting and formatting
lint = ["ruff check src/brosh tests", "ruff format --respect-gitignore src/brosh tests"]
# Format and fix style issues
fmt = ["ruff format --respect-gitignore src/brosh tests", "ruff check --fix src/brosh tests"]
fix = ["ruff check --fix --unsafe-fixes src/brosh tests", "ruff format --respect-gitignore src/brosh tests"]

# Matrix configuration to test across multiple Python versions

[[tool.hatch.envs.all.matrix]]
python = ["3.10", "3.11", "3.12"]

#------------------------------------------------------------------------------
# SPECIALIZED ENVIRONMENTS
# Additional environments for specific development tasks.
#------------------------------------------------------------------------------

# Dedicated environment for linting and code quality checks
[tool.hatch.envs.lint]
detached = true # Create a separate, isolated environment
features = ['dev'] # Use dev extras  dependencies 

# Linting environment commands
[tool.hatch.envs.lint.scripts]
# Type checking with automatic type installation
typing = "mypy --install-types --non-interactive {args:src/brosh tests}"
# Check style and format code
style = ["ruff check {args:.}", "ruff format --respect-gitignore {args:.}"]
# Format and fix style issues
fmt = ["ruff format --respect-gitignore {args:.}", "ruff check --fix {args:.}"]
fix = ["ruff check --fix --unsafe-fixes {args:.}", "ruff format --respect-gitignore {args:.}"]
# Run all ops
all = ["style", "typing", "fix"]

# Dedicated environment for testing
[tool.hatch.envs.test]
features = ['test'] # Use test extras as dependencies

# Testing environment commands
[tool.hatch.envs.test.scripts]
# Run tests in parallel
test = "python -m pytest -n auto {args:tests}"
# Run tests with coverage in parallel
test-cov = "python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/brosh --cov=tests {args:tests}"
# Run benchmarks
bench = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only"
# Run benchmarks and save results
bench-save = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json"

# Documentation environment
[tool.hatch.envs.docs]
features = ['docs']

# Documentation environment commands
[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs/source docs/build"

# GitHub Actions workflow configuration
[tool.hatch.envs.ci]
features = ['test']


[tool.hatch.envs.ci.scripts]
test = "pytest --cov=src/brosh --cov-report=xml"


#------------------------------------------------------------------------------
# CODE QUALITY TOOLS
# Configuration for linting, formatting, and code quality enforcement.
#------------------------------------------------------------------------------

#------------------------------------------------------------------------------
# COVERAGE CONFIGURATION
# Settings for test coverage measurement and reporting.
#------------------------------------------------------------------------------

# Path mapping for coverage in different environments
[tool.coverage.paths]
brosh = ["src/brosh", "*/brosh/src/brosh"]
tests = ["tests", "*/brosh/tests"]

# Coverage report configuration
[tool.coverage.report]
# Lines to exclude from coverage reporting
exclude_lines = [
    'no cov', # Custom marker to skip coverage
    'if __name__ == .__main__.:', # Script execution guard
    'if TYPE_CHECKING:', # Type checking imports and code
    'pass', # Empty pass statements
    'raise NotImplementedError', # Unimplemented method placeholders
    'raise ImportError', # Import error handling
    'except ImportError', # Import error handling
    'except KeyError', # Common error handling
    'except AttributeError', # Common error handling
    'except NotImplementedError', # Common error handling
]

[tool.coverage.run]
source_pkgs = ["brosh", "tests"]
branch = true # Measure branch coverage (if/else statements)
parallel = true # Support parallel test execution
omit = [
    "src/brosh/__about__.py",
]

#------------------------------------------------------------------------------
# MYPY CONFIGURATION
# Configuration for type checking with mypy.
#------------------------------------------------------------------------------

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[[tool.mypy.overrides]]
module = ["tests.*"]
disallow_untyped_defs = false
disallow_incomplete_defs = false

#------------------------------------------------------------------------------
# PYTEST CONFIGURATION
# Configuration for pytest, including markers, options, and benchmark settings.
#------------------------------------------------------------------------------

[tool.pytest.ini_options]
addopts = "-v --durations=10 -p no:briefcase"
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
console_output_style = "progress"
filterwarnings = ["ignore::DeprecationWarning", "ignore::UserWarning"]
log_cli = true
log_cli_level = "INFO"
markers = [
    "benchmark: marks tests as benchmarks (select with '-m benchmark')",
    "unit: mark a test as a unit test",
    "integration: mark a test as an integration test",
    "permutation: tests for permutation functionality", 
    "parameter: tests for parameter parsing",
    "prompt: tests for prompt parsing",
]
norecursedirs = [
    ".*",
    "build",
    "dist", 
    "venv",
    "__pycache__",
    "*.egg-info",
    "_private",
]
python_classes = ["Test*"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
testpaths = ["tests"]

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = "file"
save-data = true
compare = [
    "min",    # Minimum time
    "max",    # Maximum time
    "mean",   # Mean time
    "stddev", # Standard deviation
    "median", # Median time
    "iqr",    # Inter-quartile range
    "ops",    # Operations per second
    "rounds", # Number of rounds
]

#------------------------------------------------------------------------------
# RUFF CONFIGURATION
# Configuration for Ruff, including linter and formatter settings.
#------------------------------------------------------------------------------ 

# Ruff linter and formatter configuration
[tool.ruff]
target-version = "py310"
line-length = 120
exclude = [".git", ".venv", "venv", "dist", "build", "_private"]

# Linting rules configuration
[tool.ruff.lint]
# Rule sets to enable, organized by category
select = [
    # flake8 plugins and extensions
    'A', # flake8-builtins: checks for shadowed builtins
    'ARG', # flake8-unused-arguments: checks for unused function arguments
    'ASYNC', # flake8-async: checks for async/await issues
    'B', # flake8-bugbear: finds likely bugs and design problems
    'C', # flake8-comprehensions: helps write better list/dict comprehensions
    'DTZ', # flake8-datetimez: checks for datetime timezone issues
    'E', # pycodestyle errors: PEP 8 style guide errors
    'EM', # flake8-errmsg: checks for better error messages
    'F', # pyflakes: detects various errors
    'FBT', # flake8-boolean-trap: checks for boolean traps in function signatures
    'I', # isort: sorts imports
    'ICN', # flake8-import-conventions: checks for import conventions
    'ISC', # flake8-implicit-str-concat: checks for implicit string concatenation
    'LOG', # flake8-logging: checks for logging issues
    'N', # pep8-naming: checks naming conventions
    'PLC', # pylint convention: checks for convention issues
    'PLE', # pylint error: checks for errors
    'PLR', # pylint refactor: suggests refactors
    'PLW', # pylint warning: checks for suspicious code
    'PT', # flake8-pytest-style: checks pytest-specific style
    'PTH', # flake8-use-pathlib: checks for stdlib path usage vs pathlib
    'PYI', # flake8-pyi: checks stub files
    'RET', # flake8-return: checks return statement consistency
    'RSE', # flake8-raise: checks raise statements
    'RUF', # Ruff-specific rules
    'S', # flake8-bandit: checks for security issues
    'SIM', # flake8-simplify: checks for code simplification opportunities
    'T', # flake8-print: checks for print statements
    'TCH', # flake8-type-checking: helps with type-checking
    'TID', # flake8-tidy-imports: checks for tidy import statements
    'UP', # pyupgrade: checks for opportunities to use newer Python features
    'W', # pycodestyle warnings: PEP 8 style guide warnings
    'YTT', # flake8-2020: checks for misuse of sys.version or sys.version_info

]
# Rules to ignore (with reasons)
ignore = [
    'B027', # Empty method in abstract base class - sometimes needed for interfaces
    'C901', # Function is too complex - sometimes complexity is necessary
    'FBT003', # Boolean positional argument in function definition - sometimes unavoidable
    'PLR0911', # Too many return statements - sometimes needed for readability
    'PLR0912', # Too many branches - sometimes needed for complex logic
    'PLR0913', # Too many arguments - sometimes needed in APIs
    'PLR0915', # Too many statements - sometimes needed for comprehensive functions
    'PLR1714', # Consider merging multiple comparisons - sometimes less readable
    'PLW0603', # Using the global statement - sometimes necessary
    'PT013', # Pytest explicit test parameter - sometimes clearer
    'PTH123', # Path traversal - sometimes needed
    'PYI056', # Calling open() in pyi file - sometimes needed in type stubs
    'S105', # Possible hardcoded password - often false positives
    'S106', # Possible hardcoded password - often false positives
    'S107', # Possible hardcoded password - often false positives
    'S110', # try-except-pass - sometimes valid for suppressing exceptions
    'SIM102'
    # Nested if statements - sometimes more readable than combined conditions
]
# Rules that should not be automatically fixed
unfixable = [
    'F401', # Don't automatically remove unused imports - may be needed later

]
# Exclude patterns are handled in the main [tool.ruff] section

# isort configuration within Ruff
[tool.ruff.lint.isort]
known-first-party = ['brosh'] # Treat as first-party imports for sorting

# flake8-tidy-imports configuration within Ruff
[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = 'parents' # Allow relative imports within packages

# Per-file rule exceptions
[tool.ruff.lint.per-file-ignores]
# Tests can use magic values, assertions, and relative imports
'tests/**/*' = [
    'PLR2004', # Allow magic values in tests for readability
    'S101', # Allow assertions in tests
    'TID252'
    # Allow relative imports in tests for convenience
]
</file>

<file path="TODO.md">
# TODO

## ✅ COMPLETED IMPROVEMENTS (2025-06-12)

All the tasks listed below have been successfully implemented:

1. **Removed `"structured_content": null` from MCP responses** - The field has been completely removed from the MCPToolResult model
2. **Implemented comprehensive MCP result size management**:
   - Added pyoxipng dependency for PNG compression
   - All PNG files are now automatically optimized (both CLI and MCP)
   - HTML compression removes SVG content while preserving dimensions
   - Progressive compression strategy for results exceeding 1MB
   - Size checking and fallback mechanisms

### Original TODO items (now completed):

- `"structured_content": null` needs to be fully removed from MCP responses

- MCP server results cause problems if they’re too large: `result exceeds maximum length of 1048576`. We need a post-processing step before sending the result to the client: 
  - Always compress images with pyoxipng* (also when used with CLI, not just MCP).
  - If html=True, always "compress" the html content: remove all `svg` content (just keep a placeholder `svg` tag with `width` and `height` attributes), think about other ways to compress the html content to save space.
  - Check the size of the result > 1048576 bytes.
  - If it's too large and html=True, remove all HTMLs except the first one, and check the size again.
  - If it's too large, downsample each image by 50%, and check the size again.
  - If it's too large, downsample each image by 50% once more, and check the size again.
  - If it's too large, start removing the last screenshot and its metadata, and check the size again, continue until there is only one left. 



--------------------------------

*) 

# pyoxipng

[![CI](https://github.com/nfrasser/pyoxipng/actions/workflows/CI.yml/badge.svg)](https://github.com/nfrasser/pyoxipng/actions/workflows/CI.yml)
[![PyPI](https://badgen.net/pypi/v/pyoxipng)](https://pypi.org/project/pyoxipng/)

Python wrapper for multithreaded .png image file optimizer
[oxipng](https://github.com/shssoichiro/oxipng) (written in Rust). Use
`pyoxipng` to reduce the file size of your PNG images.

Jump to a section

- [Installation](#installation)
- [API](#api)

  - [optimize](#oxipngoptimizeinput-outputnone-kwargs)
  - [optimize_from_memory](#oxipngoptimize_from_memorydata-kwargs)
  - [RawImage](#oxipngrawimage)

- [Options](#options)
  - [filter](#filter)
  - [interlace](#interlace)
  - [strip](#strip)
  - [deflate](#deflate)
- [Development](#development)
- [License](#license)

## Installation

Install from PyPI:

```sh
pip install pyoxipng
```

Import in your Python code:

```py
import oxipng
```

## API

### oxipng.optimize(input, output=None, \*\*kwargs)

Optimize a file on disk.

**Parameters**:

- **input** _(str | bytes | PathLike)_ – path to input file to optimize
- **output** _(str | bytes | PathLike, optional)_ – path to optimized output result file. If not specified, overwrites input. Defaults to None
- **\*\*kwargs** – [Options](#options)

**Returns**

- None

**Raises**

- **oxipng.PngError** – optimization could not be completed

**Examples:**

Optimize a file on disk and overwrite

```py
oxipng.optimize("/path/to/image.png")
```

Optimize a file and save to a new location:

```py
oxipng.optimize("/path/to/image.png", "/path/to/image-optimized.png")
```

### oxipng.optimize_from_memory(data, \*\*kwargs)

Optimize raw data from a PNG file loaded in Python as a `bytes` object:

**Parameters**:

- **data** _(bytes)_ – raw PNG data to optimize
- **\*\*kwargs** – [Options](#options)

**Returns**

- _(bytes)_ – optimized raw PNG data

**Raises**

- **oxipng.PngError** – optimization could not be completed

**Examples:**

```py
data = ...  # bytes of png data
optimized_data = oxipng.optimize_from_memory(data)
with open("/path/to/image-optimized.png", "wb") as f:
    f.write(optimized_data)
```

### oxipng.RawImage

Create an optimized PNG file from raw image data:

```python
raw = oxipng.RawImage(data, width, height)
optimized_data = raw.create_optimized_png()
```

By default, assumes the input data is 8-bit, row-major RGBA, where every 4 bytes represents one pixel with Red-Green-Blue-Alpha channels. To interpret non-RGBA data, specify a `color_type` parameter with the `oxipng.ColorType` class:

| Method                                                       | Description                                                                                                                            |
| ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- |
| `oxipng.ColorType.grayscale(int \| None)`                    | Grayscale, with one color channel. Specify optional shade of gray that should be rendered as transparent.                              |
| `oxipng.ColorType.rgb(tuple[int, int, int])`                 | RGB, with three color channels. Specify optional color value that should be rendered as transparent.                                   |
| `oxipng.ColorType.indexed(list[[tuple[int, int, int, int]])` | Indexed, with one byte per pixel representing a color from the palette. Specify palette containing the colors used, up to 256 entries. |
| `oxipng.ColorType.grayscale_alpha()`                         | Grayscale + Alpha, with two color channels.                                                                                            |
| `oxipng.ColorType.rgba()`                                    | RGBA, with four color channels.                                                                                                        |

**Parameters:**

- **data** _(bytes | bytearray)_ – Raw image data bytes. Format depends on `color_type` and `bit_depth` parameters
- **width** _(int)_ – Width of raw image, in pixels
- **height** _(int)_ – Height of raw image, in pixels
- **color_type** _([oxipng.ColorType, optional)_ – Descriptor for color type used to represent this image. Optional, defaults to `oxipng.ColorType.rgba()`
- **bit_depth** _(int, optional)_ – Bit depth of raw image. Optional, defaults to 8

**Examples:**

Save RGB image data from a JPEG file, interpreting black pixels as transparent.

```python
from PIL import Image
import numpy as np

# Load an image file with Pillow
jpg = Image.open("/path/to/image.jpg")

# Convert to RGB numpy array
rgb_array = np.array(jpg.convert("RGB"), dtype=np.uint8)
height, width, channels = rgb_array.shape

# Create raw image with sRGB color profile
data = rgb_array.tobytes()
color_type = oxipng.ColorType.rgb((0, 0, 0))  # black is transparent
raw = oxipng.RawImage(data, width, height, color_type=color_type)
raw.add_png_chunk(b"sRGB", b"\0")

# Optimize and save
optimized = raw.create_optimized_png(level=6)
with open("/path/to/image/optimized.png", "wb") as f:
    f.write(optimized)
```

Save with data where bytes reference a color palette

```python
data = b"\0\1\2..."  # get index data
palette = [[0, 0, 0, 255], [1, 23, 234, 255], ...]
color_type = oxipng.ColorType.indexed(palette)
raw = oxipng.RawImage(data, 100, 100, color_type=color_type)
optimized = raw.create_optimized_png()
```

**Methods:**

#### add_png_chunk(name, data)

Add a png chunk, such as `b"iTXt"`, to be included in the output

**Parameters:**

- **name** _(bytes)_ – PNG chunk identifier
- **data** _(bytes | bytarray)_

**Returns:**

- None

#### add_icc_profile(data)

Add an ICC profile for the image

**Parameters:**

- **data** _(bytes)_ – ICC profile data

**Returns:**

- None

#### create_optimized_png(\*\*kwargs)

Create an optimized png from the raw image data using the options provided

**Parameters:**

- **\*\*kwargs** – [Options](#options)

**Returns:**

- _(bytes)_ optimized PNG image data

## Options

`optimize` , `optimize_from_memory` and `RawImage.create_optimized_png` accept the following options as keyword arguments.

**Example:**

```py
oxipng.optimize("/path/to/image.png", level=6, fix_errors=True, interlace=oxipng.Interlacing.Adam7)
```

| Option                 | Description                                                                                                                       | Type                              | Default                   |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- | ------------------------- |
| `level`                | Set the optimization level to an integer between 0 and 6 (inclusive)                                                              | int                               | `2`                       |
| `fix_errors`           | Attempt to fix errors when decoding the input file rather than throwing `PngError`                                                | bool                              | `False`                   |
| `force`                | Write to output even if there was no improvement in compression                                                                   | bool                              | `False`                   |
| `filter`               | Which filters to try on the file. Use Use enum values from `oxipng.RowFilter`                                                     | Sequence[[RowFilter](#filter)]    | `[RowFilter.NoOp]`        |
| `interlace`            | Whether to change the interlacing type of the file. `None` will not change current interlacing type                               | [Interlacing](#interlace) \| None | `None`                    |
| `optimize_alpha`       | Whether to allow transparent pixels to be altered to improve compression                                                          | bool                              | `False`                   |
| `bit_depth_reduction`  | Whether to attempt bit depth reduction                                                                                            | bool                              | `True`                    |
| `color_type_reduction` | Whether to attempt color type reduction                                                                                           | bool                              | `True`                    |
| `palette_reduction`    | Whether to attempt palette reduction                                                                                              | bool                              | `True`                    |
| `grayscale_reduction`  | Whether to attempt grayscale reduction                                                                                            | bool                              | `True`                    |
| `idat_recoding`        | If any type of reduction is performed, IDAT recoding will be performed regardless of this setting                                 | bool                              | `True`                    |
| `scale_16`             | Whether to forcibly reduce 16-bit to 8-bit by scaling                                                                             | bool                              | `False`                   |
| `strip`                | Which headers to strip from the PNG file, if any. Specify with `oxipng.StripChunks`                                               | [StripChunks](#strip)             | `StripChunks.none()`      |
| `deflate`              | Which DEFLATE algorithm to use. Specify with `oxipng.Deflaters`                                                                   | [Deflaters](#deflate)             | `Deflaters.libdeflater()` |
| `fast_evaluation`      | Whether to use fast evaluation to pick the best filter                                                                            | bool                              | `False`                   |
| `timeout`              | Maximum amount of time to spend (in seconds) on optimizations. Further potential optimizations skipped if the timeout is exceeded | float \| None                     | `None`                    |

### filter

Initialize a `filter` list or tuple with any of the following `oxipng.RowFilter` enum options:

- `oxipng.RowFilter.NoOp`
- `oxipng.RowFilter.Sub`
- `oxipng.RowFilter.Up`
- `oxipng.RowFilter.Average`
- `oxipng.RowFilter.Paeth`
- `oxipng.RowFilter.Bigrams`
- `oxipng.RowFilter.BigEnt`
- `oxipng.RowFilter.Brute`

### interlace

Set `interlace` to `None` to keep existing interlacing or to one of following `oxipng.Interlacing` enum options:

- `oxipng.Interlacing.Off` (interlace disabled)
- `oxipng.Interlacing.Adam7` (interlace enabled)

### strip

Initialize the `strip` option with one of the following static methods in the
`oxipng.StripChunks` class.

| Method                                      | Description                                                                                 |
| ------------------------------------------- | ------------------------------------------------------------------------------------------- |
| `oxipng.StripChunks.none()`                 | None                                                                                        |
| `oxipng.StripChunks.strip(Sequence[bytes])` | Strip chunks specified in the given list                                                    |
| `oxipng.StripChunks.safe()`                 | Strip chunks that won't affect rendering (all but cICP, iCCP, sRGB, pHYs, acTL, fcTL, fdAT) |
| `oxipng.StripChunks.keep(Sequence[bytes])`  | Strip all non-critical chunks except those in the given list                                |
| `oxipng.StripChunks.all()`                  | Strip all non-critical chunks                                                               |

### deflate

Initialize the `deflate` option with one of the following static methods in the
`oxipng.Deflaters` class.

| Method                              | Description                                                |
| ----------------------------------- | ---------------------------------------------------------- |
| `oxipng.Deflaters.libdeflater(int)` | Libdeflater with compression level [0-12]                  |
| `oxipng.Deflaters.zopfli(int)`      | Zopfli with number of compression iterations to do [1-255] |

## Development

1. Install [Rust](https://www.rust-lang.org/tools/install)
1. Install [Python 3.8+](https://www.python.org/downloads/)
1. Install [Pipenv](https://pipenv.pypa.io/en/latest/)
1. Clone this repository and navigate to it via command line
   ```sh
   git clone https://github.com/nfrasser/pyoxipng.git
   cd pyoxipng
   ```
1. Install dependencies
   ```sh
   pipenv install --dev
   ```
1. Activate the dev environment
   ```
   pipenv shell
   ```
1. Build
   ```sh
   maturin develop
   ```
1. Run tests
   ```
   pytest
   ```
1. Format code
   ```
   ruff check .
   ruff format .
   ```

## License

MIT
</file>

<file path="src/brosh/brosh.py">
#!/usr/bin/env python3
"""brosh:

Created by Adam Twardoch
"""

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from loguru import logger


@dataclass
class Config:
    """Configuration settings for brosh."""

    name: str
    value: str | int | float
    options: dict[str, Any] | None = None


def process_data(data: list[Any], config: Config | None = None, *, debug: bool = False) -> dict[str, Any]:
    """Process the input data according to configuration.

    Args:
        data: Input data to process
        config: Optional configuration settings
        debug: Enable debug mode

    Returns:
        Processed data as a dictionary

    Raises:
        ValueError: If input data is invalid
    """
    if debug:
        logger.setLevel(logging.DEBUG)
        logger.debug("Debug mode enabled")

    if not data:
        msg = "Input data cannot be empty"
        raise ValueError(msg)

    # TODO: Implement data processing logic
    result: dict[str, Any] = {}
    return result


def main() -> None:
    """Main entry point for brosh."""
    try:
        # Example usage
        config = Config(name="default", value="test", options={"key": "value"})
        result = process_data([], config=config)
        logger.info("Processing completed: %s", result)

    except Exception as e:
        logger.error("An error occurred: %s", str(e))
        raise


if __name__ == "__main__":
    main()
</file>

<file path="tests/test_package.py">
"""Test suite for brosh."""


def test_version():
    """Verify package exposes version."""
    import brosh

    assert brosh.__version__
</file>

<file path="README.md">
# brosh - Browser Screenshot Tool

A powerful browser screenshot tool that captures scrolling screenshots of webpages using Playwright's async API. Supports intelligent section identification, multiple output formats including animated PNG, and MCP (Model Context Protocol) integration.

[![Python](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Table of Contents

- [Features](#features)
- [How It Works](#how-it-works)
- [Installation](#installation)
  - [Using uv/uvx (Recommended)](#using-uvuvx-recommended)
  - [Using pip](#using-pip)
  - [Using pipx](#using-pipx)
  - [From Source](#from-source)
- [Quick Start](#quick-start)
- [Usage](#usage)
  - [Command Line Interface](#command-line-interface)
  - [MCP Server Mode](#mcp-server-mode)
  - [Python API](#python-api)
- [Command Reference](#command-reference)
  - [Global Options](#global-options)
  - [Commands](#commands)
- [Output](#output)
- [Advanced Usage](#advanced-usage)
  - [Browser Management](#browser-management)
  - [Custom Viewports](#custom-viewports)
  - [HTML Extraction](#html-extraction)
  - [Animation Creation](#animation-creation)
- [MCP Integration](#mcp-integration)
  - [What is MCP?](#what-is-mcp)
  - [Setting Up MCP Server](#setting-up-mcp-server)
  - [Configuring Claude Desktop](#configuring-claude-desktop)
- [Architecture](#architecture)
- [Development](#development)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

## Features

- **🚀 Async Playwright Integration**: Fast and reliable browser automation
- **🔍 Smart Section Detection**: Automatically identifies visible sections for descriptive filenames
- **🖼️ Multiple Formats**: PNG, JPG, and animated PNG (APNG) output
- **🌐 Browser Support**: Chrome, Edge, and Safari (macOS)
- **🔌 Remote Debugging**: Connects to existing browser sessions preserving cookies/auth
- **🤖 MCP Server**: Integrate with AI tools via Model Context Protocol
- **📄 HTML Extraction**: Optionally capture HTML content of visible elements
- **📝 Text Extraction**: Automatically converts visible content to Markdown text
- **📐 Flexible Scrolling**: Configurable scroll steps and starting positions
- **🎯 Precise Control**: Set viewport size, zoom level, and output scaling
- **🔄 Automatic Retries**: Robust error handling with configurable retry logic

## How It Works

**brosh** works by:

1. **Browser Connection**: Connects to an existing browser in debug mode or launches a new instance
2. **Page Navigation**: Navigates to the specified URL and waits for content to load
3. **Smart Scrolling**: Scrolls through the page in configurable steps, capturing screenshots
4. **Section Detection**: Identifies visible headers and elements to create meaningful filenames
5. **Image Processing**: Applies scaling, format conversion, and creates animations if requested
6. **Output Organization**: Saves screenshots with descriptive names including domain, timestamp, and section

The tool is especially useful for:
- **Documentation**: Capturing long technical documentation or API references
- **QA Testing**: Visual regression testing and bug reporting
- **Content Archival**: Preserving web content with full page captures
- **Design Reviews**: Sharing complete page designs with stakeholders
- **AI Integration**: Providing visual context to language models via MCP

## Installation

### Using uv/uvx (Recommended)

[uv](https://github.com/astral-sh/uv) is a fast Python package manager that replaces pip, pip-tools, pipx, poetry, pyenv, and virtualenv.

```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Run brosh directly with uvx (no installation needed)
uvx brosh shot "https://example.com"

# Or install globally
uv tool install brosh

# Install with all extras
uv tool install "brosh[all]"
```

### Using pip

```bash
# Basic installation
pip install brosh

# With all optional dependencies
pip install "brosh[all]"
```

### Using pipx

[pipx](https://pipx.pypa.io/) installs Python applications in isolated environments.

```bash
# Install pipx (if not already installed)
python -m pip install --user pipx
python -m pipx ensurepath

# Install brosh
pipx install brosh
```

### From Source

```bash
git clone https://github.com/twardoch/brosh.git
cd brosh
pip install -e ".[all]"
```

### Install Playwright Browsers

After installation, you need to install the browser drivers:

```bash
playwright install
```

## Quick Start

```bash
# Capture a single webpage
brosh shot "https://example.com"

# Start browser in debug mode for better performance
brosh run
brosh shot "https://example.com"

# Create an animated PNG showing the scroll
brosh shot "https://example.com" --format apng

# Capture with custom viewport
brosh --width 1920 --height 1080 shot "https://example.com"

# Extract HTML content
brosh shot "https://example.com" --html --json > content.json
```

## Usage

### Command Line Interface

brosh provides a Fire-based CLI with intuitive commands and options.

#### Basic Screenshot Capture

```bash
# Simple capture
brosh shot "https://example.com"

# Capture with custom settings
brosh --width 1920 --height 1080 --zoom 125 shot "https://example.com"

# Capture entire page height (no viewport limit)
brosh --height -1 shot "https://example.com"

# Save to specific directory
brosh --output_dir ~/Screenshots shot "https://example.com"

# Organize by domain
brosh --subdirs shot "https://example.com"
```

#### Advanced Capture Options

```bash
# Start from specific element
brosh shot "https://docs.python.org" --from_selector "#functions"

# Limit number of screenshots
brosh shot "https://example.com" --max_frames 5

# Adjust scroll step (percentage of viewport)
brosh shot "https://example.com" --scroll_step 50

# Scale output images
brosh shot "https://example.com" --scale 75

# Create animated PNG
brosh shot "https://example.com" --format apng --anim_spf 1.0

# Extract visible HTML
brosh shot "https://example.com" --html --json > page_content.json
```

### MCP Server Mode

Run as an MCP server for AI tool integration:

```bash
# Using the dedicated command
brosh-mcp

# Or via the main command
brosh mcp
```

### Python API

```python
import asyncio
from brosh import BrowserScreenshotTool

async def capture_screenshots():
    tool = BrowserScreenshotTool(verbose=True)
    
    # Basic capture
    screenshots = await tool.capture(
        url="https://example.com",
        width=1920,
        height=1080,
        scroll_step=100,
        format="png"
    )
    
    print(f"Captured {len(screenshots)} screenshots")
    for path in screenshots:
        print(f"  - {path}")
    
    # Capture with HTML extraction
    result = await tool.capture(
        url="https://example.com",
        html=True,
        max_frames=3,
        from_selector="#main-content"
    )
    
    # Result is a dict with paths as keys and HTML/selectors as values
    for path, data in result.items():
        print(f"\nScreenshot: {path}")
        print(f"Selector: {data['selector']}")
        print(f"HTML preview: {data['html'][:200]}...")

# Run the async function
asyncio.run(capture_screenshots())
```

## Command Reference

### Global Options

These options can be used with any command:

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `--app` | str | auto-detect | Browser to use: `chrome`, `edge`, `safari` |
| `--width` | int | screen width | Viewport width in pixels |
| `--height` | int | screen height | Viewport height in pixels (-1 for full page) |
| `--zoom` | int | 100 | Zoom level percentage (10-500) |
| `--output_dir` | str | ~/Pictures | Output directory for screenshots |
| `--subdirs` | bool | False | Create subdirectories for each domain |
| `--verbose` | bool | False | Enable debug logging |

### Commands

#### `run` - Start Browser in Debug Mode

```bash
brosh [--app BROWSER] run [--force_run]
```

Starts the browser in remote debugging mode for better performance with multiple captures.

**Options:**
- `--force_run`: Force restart even if already running

#### `quit` - Quit Browser

```bash
brosh [--app BROWSER] quit
```

Closes the browser started in debug mode.

#### `shot` - Capture Screenshots

```bash
brosh [OPTIONS] shot URL [SHOT_OPTIONS]
```

**Required:**
- `URL`: The webpage URL to capture

**Shot Options:**
| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `--scroll_step` | int | 100 | Scroll step as % of viewport height (10-200) |
| `--scale` | int | 100 | Scale output images by % (10-200) |
| `--format` | str | png | Output format: `png`, `jpg`, `apng` |
| `--anim_spf` | float | 0.5 | Seconds per frame for APNG |
| `--html` | bool | False | Extract HTML content of visible elements |
| `--json` | bool | False | Output results as JSON |
| `--max_frames` | int | 0 | Maximum screenshots (0 = all) |
| `--from_selector` | str | "" | CSS selector to start capture from |

#### `mcp` - Run MCP Server

```bash
brosh mcp
```

Starts the MCP server for AI tool integration.

## Output

### File Naming Convention

Screenshots are saved with descriptive filenames:

```
{domain}-{timestamp}-{scroll_position}-{section}.{format}
```

**Example:**
```
github_com-250612-185234-00500-readme.png
│         │              │     │
│         │              │     └── Section identifier
│         │              └──────── Scroll position (0-9999)
│         └─────────────────────── Timestamp (YYMMDD-HHMMSS)
└───────────────────────────────── Domain name
```

### Output Formats

- **PNG**: Lossless compression, best quality (default)
- **JPG**: Smaller file size, good for photos
- **APNG**: Animated PNG showing the scroll sequence

### JSON Output

The tool now always extracts text content from visible elements. When using `--json`:

**Default output (without --html):**
```json
{
  "/path/to/screenshot1.png": {
    "selector": "main.content",
    "text": "# Main Content\n\nThis is the extracted text in Markdown format..."
  }
}
```

**With --html flag:**
```json
{
  "/path/to/screenshot1.png": {
    "selector": "main.content",
    "html": "<main class='content'>...</main>",
    "text": "# Main Content\n\nThis is the extracted text in Markdown format..."
  }
}
```

The `text` field contains the visible content converted to Markdown format using html2text, making it easy to process the content programmatically.

## Advanced Usage

### Browser Management

brosh can connect to your existing browser session, preserving cookies, authentication, and extensions:

```bash
# Start Chrome in debug mode
brosh --app chrome run

# Your regular browsing session remains active
# brosh connects to it for screenshots

# Take screenshots with your logged-in session
brosh shot "https://github.com/notifications"

# Quit when done
brosh --app chrome quit
```

### Custom Viewports

Simulate different devices by setting viewport dimensions:

```bash
# Desktop - 4K
brosh --width 3840 --height 2160 shot "https://example.com"

# Desktop - 1080p
brosh --width 1920 --height 1080 shot "https://example.com"

# Tablet
brosh --width 1024 --height 768 shot "https://example.com"

# Mobile
brosh --width 375 --height 812 shot "https://example.com"
```

### HTML Extraction

Extract the HTML content of visible elements for each screenshot:

```bash
# Get HTML with screenshots
brosh shot "https://example.com" --html --json > content.json

# Process the extracted content
cat content.json | jq 'to_entries | .[] | {
  screenshot: .key,
  wordCount: (.value.html | split(" ") | length)
}'
```

### Animation Creation

Create smooth animations showing page scroll:

```bash
# Standard animation (0.5 seconds per frame)
brosh shot "https://example.com" --format apng

# Faster animation
brosh shot "https://example.com" --format apng --anim_spf 0.2

# Slower, more detailed
brosh shot "https://example.com" --format apng --anim_spf 1.0 --scroll_step 50
```

## MCP Integration

### What is MCP?

The Model Context Protocol (MCP) is an open standard that enables seamless integration between AI applications and external data sources or tools. brosh implements an MCP server that allows AI assistants like Claude to capture and analyze web content.

### Setting Up MCP Server

#### 1. Using uvx (Recommended)

```bash
# Run directly without installation
uvx brosh-mcp

# Or install as a tool
uv tool install brosh
uvx brosh-mcp
```

#### 2. Configure Claude Desktop

Add brosh to your Claude Desktop configuration:

**macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows:** `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "brosh": {
      "command": "uvx",
      "args": ["brosh-mcp"],
      "env": {
        "FASTMCP_LOG_LEVEL": "INFO"
      }
    }
  }
}
```

**Note:** If you encounter issues with uvx, you can use the full path to brosh-mcp:

```json
{
  "mcpServers": {
    "brosh": {
      "command": "/path/to/python/bin/brosh-mcp",
      "args": [],
      "type": "stdio"
    }
  }
}
```

To find the full path:
```bash
# On Unix-like systems
which brosh-mcp

# Or with Python
python -c "import shutil; print(shutil.which('brosh-mcp'))"
```

#### 3. Alternative Configurations

**Using Python directly:**
```json
{
  "mcpServers": {
    "brosh": {
      "command": "python",
      "args": ["-m", "brosh", "mcp"]
    }
  }
}
```

**Using specific Python path:**
```json
{
  "mcpServers": {
    "brosh": {
      "command": "/usr/local/bin/python3",
      "args": ["-m", "brosh", "mcp"]
    }
  }
}
```

### Using with Claude

Once configured, you can ask Claude to:

- "Take a screenshot of python.org documentation"
- "Capture the entire React homepage with animations"
- "Get screenshots of the GitHub trending page and extract the visible HTML"
- "Show me what the Hacker News homepage looks like"

Claude will use brosh to capture the screenshots and can analyze the visual content or extracted HTML.

## Architecture

The project is organized into modular components:

```
src/brosh/
├── __init__.py      # Package exports
├── __main__.py      # CLI entry point
├── cli.py           # Command-line interface
├── tool.py          # Main screenshot tool
├── browser.py       # Browser management
├── capture.py       # Screenshot capture logic
├── image.py         # Image processing
├── models.py        # Data models
└── mcp.py           # MCP server implementation
```

### Key Components

- **BrowserManager**: Handles browser detection, launching, and connection
- **CaptureManager**: Manages scrolling and screenshot capture
- **ImageProcessor**: Handles image scaling, conversion, and animation
- **BrowserScreenshotTool**: Orchestrates the capture process
- **BrowserScreenshotCLI**: Provides the command-line interface

## Development

### Setup Development Environment

```bash
# Clone the repository
git clone https://github.com/twardoch/brosh.git
cd brosh

# Install with development dependencies
pip install -e ".[dev,test,all]"

# Install pre-commit hooks
pre-commit install
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/brosh --cov-report=term-missing

# Run specific test
pytest tests/test_capture.py -v
```

### Code Quality

```bash
# Format code
ruff format src/brosh tests

# Lint code
ruff check src/brosh tests

# Type checking
mypy src/brosh
```

### Building Documentation

```bash
# Install docs dependencies
pip install -e ".[docs]"

# Build documentation
sphinx-build -b html docs/source docs/build
```

## Troubleshooting

### Common Issues

#### Browser Not Found

**Error:** "Could not find chrome installation"

**Solution:** Ensure Chrome/Edge/Safari is installed in the default location, or specify the browser explicitly:
```bash
brosh --app edge shot "https://example.com"
```

#### Connection Timeout

**Error:** "Failed to connect to browser"

**Solution:** Start the browser in debug mode first:
```bash
brosh run
# Then in another terminal:
brosh shot "https://example.com"
```

#### Screenshot Timeout

**Error:** "Screenshot timeout for position X"

**Solution:** Increase the timeout or reduce the page complexity:
```bash
# Simpler format
brosh shot "https://example.com" --format jpg

# Fewer screenshots
brosh shot "https://example.com" --scroll_step 200
```

#### Permission Denied

**Error:** "Permission denied" when saving screenshots

**Solution:** Check the output directory permissions or use a different directory:
```bash
brosh --output_dir /tmp/screenshots shot "https://example.com"
```

### Debug Mode

Enable verbose logging to troubleshoot issues:

```bash
brosh --verbose shot "https://example.com"
```

### Platform-Specific Notes

#### macOS
- Safari requires enabling "Allow Remote Automation" in Develop menu
- Retina displays are automatically detected and handled

#### Windows
- Run as administrator if you encounter permission issues
- Chrome/Edge must be installed in default Program Files location

#### Linux
- Install additional dependencies: `sudo apt-get install libnss3 libxss1`
- Headless mode may be required on servers without display

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### Development Process

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Code Style

- Follow PEP 8 guidelines
- Use type hints for all functions
- Add docstrings to all public functions
- Keep functions focused and modular
- Write tests for new features

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Credits

Created by [Adam Twardoch](https://github.com/twardoch)

## Acknowledgments

- Built with [Playwright](https://playwright.dev/) for reliable browser automation
- Uses [Fire](https://github.com/google/python-fire) for the CLI interface
- Implements [FastMCP](https://github.com/jlowin/fastmcp) for Model Context Protocol support
- Inspired by various web scraping and screenshot tools
</file>

<file path="src/brosh/mcp.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fastmcp", "platformdirs", "pydantic", "loguru"]
# ///
# this_file: src/brosh/mcp.py

"""MCP server implementation for brosh.

This module provides FastMCP server functionality for capturing browser
screenshots using the brosh tool. It enables AI tools to request and receive
webpage captures with visual context and optional HTML/text content.
"""

import asyncio
import base64
import concurrent.futures
from collections.abc import Mapping
from pathlib import Path
from typing import Annotated

from loguru import logger
from platformdirs import user_pictures_dir
from pydantic import Field
from pydantic.networks import AnyUrl

try:
    from playwright.async_api import Error as PlaywrightError
except ImportError:
    PlaywrightError = Exception

from fastmcp import FastMCP

from .models import (
    ImageFormat,
    MCPImageContent,
    MCPScreenshotResult,
    MCPTextContent,
    MCPToolResult,
)
from .optimize import (
    calculate_result_size,
    compress_html_content,
    downsample_png_data,
    optimize_png_file,
)
from .tool import BrowserScreenshotTool


def run_mcp_server() -> None:
    """Get a screenshot of a webpage in vertical slices together with text and/or HTML content."""
    mcp = FastMCP(
        name="Brosh Web Capture",
        instructions=("Get a screenshot of a webpage in vertical slices together with text and/or HTML content."),
    )

    @mcp.tool
    async def see_webpage(
        url: Annotated[AnyUrl, Field(description="The URL to capture")],
        zoom: Annotated[
            int,
            Field(
                default=100,
                description="Zoom level in %",
                ge=10,
                le=500,
            ),
        ],
        width: Annotated[
            int,
            Field(
                default=0,
                description="Width in pixels (0: screen width)",
            ),
        ],
        height: Annotated[
            int,
            Field(
                default=0,
                description="Height in pixels (0: screen height, -1: full page height)",
            ),
        ],
        from_selector: Annotated[
            str,
            Field(
                default="",
                description="CSS selector to start from (empty: page top)",
            ),
        ],
        scroll_step: Annotated[
            int,
            Field(
                default=100,
                description="Scroll step in % of height",
                ge=10,
                le=200,
            ),
        ],
        max_frames: Annotated[
            int,
            Field(
                default=0,
                description="Max vertical scroll shots (0: unlimited)",
            ),
        ],
        app: Annotated[
            str,
            Field(
                default="",
                description="Browser to use (default: chrome)",
            ),
        ],
        scale: Annotated[
            int,
            Field(
                default=50,
                description="Output image downsample %",
                ge=5,
                le=100,
            ),
        ],
        output_dir: Annotated[
            Path,
            Field(
                default=Path(user_pictures_dir()),
                description="Output folder for screenshots",
            ),
        ],
        subdirs: Annotated[
            bool,
            Field(
                default=False,
                description="Create per-domain subfolders",
            ),
        ],
        format: Annotated[
            str,
            Field(
                default="png",
                description="Output format: png (default), jpg, apng",
            ),
        ],
        anim_spf: Annotated[
            float,
            Field(
                default=0.5,
                description="APNG seconds per frame",
                ge=0.1,
                le=10.0,
            ),
        ],
        html: Annotated[
            bool,
            Field(
                default=False,
                description="Get visible HTML code for each screenshot",
            ),
        ],
    ) -> MCPToolResult:
        """Get screenshots and text or HTML from a webpage.

        Call this tool when you must *see* the page as a user would (e.g. verify design). Tweak zoom, viewport size, scroll_step, max_frames, format for optimal results. It scrolls from the given CSS selector (or top).

        Args:
            url: The webpage URL to capture
            zoom: Browser zoom level (10-500%)
            width: Viewport width (0: screen width)
            height: Viewport height (0: screen height, -1: full page)
            from_selector: Starting CSS selector (empty: page top)
            scroll_step: Vertical scroll increment (10-200% of height)
            max_frames: Maximum scroll captures (0: unlimited)
            app: Browser to use (default: chrome)
            scale: Output image scale (10-200%)
            output_dir: Screenshot save location
            subdirs: Create domain-based subdirectories
            format: Output image format (png/jpg/apng)
            anim_spf: APNG animation speed (0.1-10.0 seconds)
            html: Include visible HTML content

        Returns:
            {
                "<file_path>": {
                    "image": Image(...),      # screenshot or APNG frame
                    "selector": "<css>",      # DOM element in view
                    "text": "<markdown>",     # readable text found
                    "html": "<raw_html>"      # optional, if html=True
                },
                ...
            }

        Raises:
            Exception: If screenshot capture or processing fails
        """
        tool = BrowserScreenshotTool()
        logger.debug(f"MCP see_webpage called for url={url} html={html}")
        try:
            # Run capture in a separate event loop to avoid conflicts
            # This is similar to how the CLI works (asyncio.run)
            def run_capture():
                return asyncio.run(
                    tool.capture(
                        url=url,
                        zoom=zoom,
                        width=width,
                        height=height,
                        scroll_step=scroll_step,
                        scale=scale,
                        app=app,
                        output_dir=output_dir,
                        subdirs=subdirs,
                        mcp=False,  # Never recurse into MCP mode
                        format=format,
                        anim_spf=anim_spf,
                        html=html,  # Get both selector and HTML if True
                        max_frames=max_frames,
                        from_selector=from_selector,
                    )
                )

            # Execute in thread executor with timeout
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = loop.run_in_executor(executor, run_capture)
                result = await asyncio.wait_for(future, timeout=60.0)

            logger.debug("MCP see_webpage capture completed successfully")
        except TimeoutError:
            logger.error("MCP see_webpage: capture timed out")
            return MCPToolResult(content=[MCPTextContent(text="Error: Screenshot capture timed out.")])
        except asyncio.InvalidStateError as e:
            logger.error(f"MCP see_webpage: InvalidStateError: {e}")
            return MCPToolResult(content=[MCPTextContent(text=f"Error: InvalidStateError: {e}")])
        except PlaywrightError as e:
            logger.error(f"MCP see_webpage: Playwright error: {e}")
            return MCPToolResult(content=[MCPTextContent(text=f"Error: Playwright error: {e}")])
        except Exception as e:
            logger.error(f"MCP see_webpage: Unexpected error: {e}")
            return MCPToolResult(content=[MCPTextContent(text=f"Error: Unexpected error: {e}")])
        # Process capture results for MCP response
        if isinstance(result, dict):
            mcp_result = _process_dict_result(result, format)
        else:
            mcp_result = _process_list_result(result, format)

        # Post-process to ensure size limits
        return _post_process_result(mcp_result, has_html=html)

    mcp.run()


def _process_dict_result(
    result: Mapping[str, str | dict[str, str]],
    format: ImageFormat,
) -> MCPToolResult:
    """Process dictionary-format capture results.

    Args:
        result: Raw capture results with paths as keys
        format: Image format used for the captures

    Returns:
        Processed results with image data and metadata
    """
    content_items = []

    for path, value in result.items():
        try:
            screenshot = _create_screenshot_result(path, format)
            # Add metadata from the capture result
            if isinstance(value, dict):
                screenshot.selector = value.get("selector", "body")
                if "html" in value:
                    # Compress HTML to save space
                    screenshot.html = compress_html_content(value["html"])
                if "text" in value:
                    screenshot.text = value["text"]
            else:
                # Legacy format - value is just the selector
                screenshot.selector = str(value)
            # Add the image content item
            content_items.append(screenshot.image)
            # Add text metadata item
            content_items.append(MCPTextContent(text=screenshot.metadata_json(path)))
        except Exception as e:
            logger.error(f"Failed to process {path}: {e}")
            continue
    return MCPToolResult(content=content_items)


def _process_list_result(
    result: list[str],
    format: ImageFormat,
) -> MCPToolResult:
    """Process list-format capture results.

    Args:
        result: List of screenshot file paths
        format: Image format used for the captures

    Returns:
        Processed results with image data
    """
    content_items = []
    for path in result:
        try:
            screenshot = _create_screenshot_result(path, format)
            content_items.append(screenshot.image)
            content_items.append(MCPTextContent(text=screenshot.metadata_json(path)))
        except Exception as e:
            logger.error(f"Failed to process {path}: {e}")
            continue
    return MCPToolResult(content=content_items)


def _create_screenshot_result(
    path: str,
    format: ImageFormat,
    optimize: bool = True,
) -> MCPScreenshotResult:
    """Create a screenshot result from a file path.

    Args:
        path: Path to the image file
        format: Image format used for the capture
        optimize: Whether to optimize PNG files

    Returns:
        Screenshot result with image data and metadata

    Raises:
        Exception: If image file cannot be read
    """
    # Optimize PNG files if requested
    if optimize and format == ImageFormat.PNG:
        img_bytes = optimize_png_file(path, level=6, preserve_file=True)
    else:
        with open(path, "rb") as f:
            img_bytes = f.read()

    # Create image content item
    image = MCPImageContent(
        data=base64.b64encode(img_bytes).decode(),
        mime_type=format.mime_type,
    )

    return MCPScreenshotResult(
        image=image,
        selector="body",
        text=None,
        html=None,
    )


def _post_process_result(result: MCPToolResult, has_html: bool = False) -> MCPToolResult:
    """Post-process MCP result to ensure it fits within size limits.

    Args:
        result: The MCPToolResult to process
        has_html: Whether the result contains HTML content

    Returns:
        Processed MCPToolResult that fits within size limits
    """
    MAX_SIZE = 1048576  # 1MB limit

    # Check initial size
    result_dict = result.model_dump()
    size = calculate_result_size(result_dict)

    if size <= MAX_SIZE:
        return result

    logger.warning(f"Result size {size} exceeds limit {MAX_SIZE}, applying compression")

    # Step 1: If has HTML, remove all HTML except the first one
    if has_html and len(result.content) > 2:
        new_content = []
        html_found = False

        for _i, item in enumerate(result.content):
            if isinstance(item, MCPTextContent):
                # Parse the text to check if it contains HTML
                try:
                    import json

                    data = json.loads(item.text)
                    if isinstance(data, dict):
                        for path, metadata in data.items():
                            if "html" in metadata and html_found:
                                # Remove HTML from subsequent items
                                metadata.pop("html", None)
                                item = MCPTextContent(text=json.dumps({path: metadata}))
                            elif "html" in metadata:
                                html_found = True
                except:
                    pass
            new_content.append(item)

        result = MCPToolResult(content=new_content)
        size = calculate_result_size(result.model_dump())

        if size <= MAX_SIZE:
            return result

    # Step 2: Downsample images by 50%
    new_content = []
    for item in result.content:
        if isinstance(item, MCPImageContent):
            try:
                # Decode, downsample, re-encode
                img_data = base64.b64decode(item.data)
                downsampled = downsample_png_data(img_data, scale_factor=0.5)
                item = MCPImageContent(
                    data=base64.b64encode(downsampled).decode(),
                    mime_type=item.mime_type,
                )
            except Exception as e:
                logger.error(f"Failed to downsample image: {e}")
        new_content.append(item)

    result = MCPToolResult(content=new_content)
    size = calculate_result_size(result.model_dump())

    if size <= MAX_SIZE:
        return result

    # Step 3: Downsample again by 50%
    new_content = []
    for item in result.content:
        if isinstance(item, MCPImageContent):
            try:
                img_data = base64.b64decode(item.data)
                downsampled = downsample_png_data(img_data, scale_factor=0.5)
                item = MCPImageContent(
                    data=base64.b64encode(downsampled).decode(),
                    mime_type=item.mime_type,
                )
            except Exception as e:
                logger.error(f"Failed to downsample image again: {e}")
        new_content.append(item)

    result = MCPToolResult(content=new_content)
    size = calculate_result_size(result.model_dump())

    if size <= MAX_SIZE:
        return result

    # Step 4: Start removing screenshots from the end
    while len(result.content) > 2 and size > MAX_SIZE:
        # Remove last image and its metadata
        new_content = result.content[:-2]
        result = MCPToolResult(content=new_content)
        size = calculate_result_size(result.model_dump())

    return result


def main() -> None:
    """Run the MCP server."""
    run_mcp_server()


if __name__ == "__main__":
    main()
</file>

</files>
